{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import multiprocessing\n",
    "import copy\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib notebook\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from stable_baselines import bench, logger\n",
    "from stable_baselines.common import set_global_seeds\n",
    "from stable_baselines.common import explained_variance, ActorCriticRLModel, tf_util, SetVerbosity, TensorboardWriter\n",
    "from stable_baselines.common.runners import AbstractEnvRunner\n",
    "from stable_baselines.common.policies import LstmPolicy, ActorCriticPolicy\n",
    "from stable_baselines.a2c.utils import total_episode_reward_logger\n",
    "from stable_baselines.ppo2 import PPO2\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines.common.vec_env.vec_normalize import VecNormalize\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "\n",
    "from pathlib import Path\n",
    "home = str(Path.home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO2_Mod(ActorCriticRLModel):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (GPU version).\n",
    "    Paper: https://arxiv.org/abs/1707.06347\n",
    "\n",
    "    :param policy: (ActorCriticPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, CnnLstmPolicy, ...)\n",
    "    :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n",
    "    :param gamma: (float) Discount factor\n",
    "    :param n_steps: (int) The number of steps to run for each environment per update\n",
    "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "    :param ent_coef: (float) Entropy coefficient for the loss caculation\n",
    "    :param learning_rate: (float or callable) The learning rate, it can be a function\n",
    "    :param vf_coef: (float) Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: (float) The maximum value for the gradient clipping\n",
    "    :param lam: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "    :param nminibatches: (int) Number of training minibatches per update. For recurrent policies,\n",
    "        the number of environments run in parallel should be a multiple of nminibatches.\n",
    "    :param noptepochs: (int) Number of epoch when optimizing the surrogate\n",
    "    :param cliprange: (float or callable) Clipping parameter, it can be a function\n",
    "    :param verbose: (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n",
    "    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n",
    "    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, policy, env, gamma=0.99, n_steps=128, ent_coef=0.01, learning_rate=2.5e-4, vf_coef=0.5,\n",
    "                 max_grad_norm=0.5, lam=0.95, nminibatches=4, noptepochs=4, cliprange=0.2, verbose=0,\n",
    "                 tensorboard_log=None, _init_setup_model=True):\n",
    "\n",
    "        super(PPO2_Mod, self).__init__(policy=policy, env=env, verbose=verbose, requires_vec_env=True,\n",
    "                                   _init_setup_model=_init_setup_model)\n",
    "\n",
    "        if isinstance(learning_rate, float):\n",
    "            learning_rate = constfn(learning_rate)\n",
    "        else:\n",
    "            assert callable(learning_rate)\n",
    "        if isinstance(cliprange, float):\n",
    "            cliprange = constfn(cliprange)\n",
    "        else:\n",
    "            assert callable(cliprange)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cliprange = cliprange\n",
    "        self.n_steps = n_steps\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.nminibatches = nminibatches\n",
    "        self.noptepochs = noptepochs\n",
    "        self.tensorboard_log = tensorboard_log\n",
    "\n",
    "        self.graph = None\n",
    "        self.sess = None\n",
    "        self.action_ph = None\n",
    "        self.advs_ph = None\n",
    "        self.rewards_ph = None\n",
    "        self.old_neglog_pac_ph = None\n",
    "        self.old_vpred_ph = None\n",
    "        self.learning_rate_ph = None\n",
    "        self.clip_range_ph = None\n",
    "        self.entropy = None\n",
    "        self.vf_loss = None\n",
    "        self.pg_loss = None\n",
    "        self.approxkl = None\n",
    "        self.clipfrac = None\n",
    "        self.params = None\n",
    "        self._train = None\n",
    "        self.loss_names = None\n",
    "        self.train_model = None\n",
    "        self.act_model = None\n",
    "        self.step = None\n",
    "        self.proba_step = None\n",
    "        self.value = None\n",
    "        self.initial_state = None\n",
    "        self.n_batch = None\n",
    "        self.summary = None\n",
    "        self.episode_reward = None\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self.setup_model()\n",
    "\n",
    "    def setup_model(self):\n",
    "        with SetVerbosity(self.verbose):\n",
    "\n",
    "            assert issubclass(self.policy, ActorCriticPolicy), \"Error: the input policy for the PPO2 model must be \" \\\n",
    "                                                               \"an instance of common.policies.ActorCriticPolicy.\"\n",
    "\n",
    "            self.n_batch = self.n_envs * self.n_steps\n",
    "\n",
    "            n_cpu = multiprocessing.cpu_count()\n",
    "            if sys.platform == 'darwin':\n",
    "                n_cpu //= 2\n",
    "\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "                self.sess = tf_util.make_session(num_cpu=n_cpu, graph=self.graph)\n",
    "\n",
    "                n_batch_step = None\n",
    "                n_batch_train = None\n",
    "                if issubclass(self.policy, LstmPolicy):\n",
    "                    assert self.n_envs % self.nminibatches == 0, \"For recurrent policies, \"\\\n",
    "                        \"the number of environments run in parallel should be a multiple of nminibatches.\"\n",
    "                    n_batch_step = self.n_envs\n",
    "                    n_batch_train = self.n_batch // self.nminibatches\n",
    "\n",
    "                act_model = self.policy(self.sess, self.observation_space, self.action_space, self.n_envs, 1,\n",
    "                                        n_batch_step, reuse=False)\n",
    "                with tf.variable_scope(\"train_model\", reuse=True,\n",
    "                                       custom_getter=tf_util.outer_scope_getter(\"train_model\")):\n",
    "                    train_model = self.policy(self.sess, self.observation_space, self.action_space,\n",
    "                                              self.n_envs // self.nminibatches, self.n_steps, n_batch_train,\n",
    "                                              reuse=True)\n",
    "\n",
    "                with tf.variable_scope(\"loss\", reuse=False):\n",
    "                    self.action_ph = train_model.pdtype.sample_placeholder([None], name=\"action_ph\")\n",
    "                    self.advs_ph = tf.placeholder(tf.float32, [None], name=\"advs_ph\")\n",
    "                    self.rewards_ph = tf.placeholder(tf.float32, [None], name=\"rewards_ph\")\n",
    "                    self.old_neglog_pac_ph = tf.placeholder(tf.float32, [None], name=\"old_neglog_pac_ph\")\n",
    "                    self.old_vpred_ph = tf.placeholder(tf.float32, [None], name=\"old_vpred_ph\")\n",
    "                    self.learning_rate_ph = tf.placeholder(tf.float32, [], name=\"learning_rate_ph\")\n",
    "                    self.clip_range_ph = tf.placeholder(tf.float32, [], name=\"clip_range_ph\")\n",
    "\n",
    "                    neglogpac = train_model.proba_distribution.neglogp(self.action_ph)\n",
    "                    self.entropy = tf.reduce_mean(train_model.proba_distribution.entropy())\n",
    "\n",
    "                    vpred = train_model._value\n",
    "                    vpredclipped = self.old_vpred_ph + tf.clip_by_value(\n",
    "                        train_model._value - self.old_vpred_ph, - self.clip_range_ph, self.clip_range_ph)\n",
    "                    vf_losses1 = tf.square(vpred - self.rewards_ph)\n",
    "                    vf_losses2 = tf.square(vpredclipped - self.rewards_ph)\n",
    "                    self.vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))\n",
    "                    ratio = tf.exp(self.old_neglog_pac_ph - neglogpac)\n",
    "                    pg_losses = -self.advs_ph * ratio\n",
    "                    pg_losses2 = -self.advs_ph * tf.clip_by_value(ratio, 1.0 - self.clip_range_ph, 1.0 +\n",
    "                                                                  self.clip_range_ph)\n",
    "                    self.pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))\n",
    "                    self.approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - self.old_neglog_pac_ph))\n",
    "                    self.clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), self.clip_range_ph)))\n",
    "                    loss = self.pg_loss - self.entropy * self.ent_coef + self.vf_loss * self.vf_coef\n",
    "\n",
    "                    tf.summary.scalar('entropy_loss', self.entropy)\n",
    "                    tf.summary.scalar('policy_gradient_loss', self.pg_loss)\n",
    "                    tf.summary.scalar('value_function_loss', self.vf_loss)\n",
    "                    tf.summary.scalar('approximate_kullback-leiber', self.approxkl)\n",
    "                    tf.summary.scalar('clip_factor', self.clipfrac)\n",
    "                    tf.summary.scalar('loss', loss)\n",
    "\n",
    "                    with tf.variable_scope('model'):\n",
    "                        self.params = tf.trainable_variables()\n",
    "                    grads = tf.gradients(loss, self.params)\n",
    "                    if self.max_grad_norm is not None:\n",
    "                        grads, _grad_norm = tf.clip_by_global_norm(grads, self.max_grad_norm)\n",
    "                    grads = list(zip(grads, self.params))\n",
    "                trainer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph, epsilon=1e-5)\n",
    "                self._train = trainer.apply_gradients(grads)\n",
    "\n",
    "                self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']\n",
    "\n",
    "                with tf.variable_scope(\"input_info\", reuse=False):\n",
    "                    tf.summary.scalar('discounted_rewards', tf.reduce_mean(self.rewards_ph))\n",
    "                    tf.summary.histogram('discounted_rewards', self.rewards_ph)\n",
    "                    tf.summary.scalar('learning_rate', tf.reduce_mean(self.learning_rate_ph))\n",
    "                    tf.summary.histogram('learning_rate', self.learning_rate_ph)\n",
    "                    tf.summary.scalar('advantage', tf.reduce_mean(self.advs_ph))\n",
    "                    tf.summary.histogram('advantage', self.advs_ph)\n",
    "                    tf.summary.scalar('clip_range', tf.reduce_mean(self.clip_range_ph))\n",
    "                    tf.summary.histogram('clip_range', self.clip_range_ph)\n",
    "                    tf.summary.scalar('old_neglog_action_probabilty', tf.reduce_mean(self.old_neglog_pac_ph))\n",
    "                    tf.summary.histogram('old_neglog_action_probabilty', self.old_neglog_pac_ph)\n",
    "                    tf.summary.scalar('old_value_pred', tf.reduce_mean(self.old_vpred_ph))\n",
    "                    tf.summary.histogram('old_value_pred', self.old_vpred_ph)\n",
    "                    if len(self.observation_space.shape) == 3:\n",
    "                        tf.summary.image('observation', train_model.obs_ph)\n",
    "                    else:\n",
    "                        tf.summary.histogram('observation', train_model.obs_ph)\n",
    "\n",
    "                self.train_model = train_model\n",
    "                self.act_model = act_model\n",
    "                self.step = act_model.step\n",
    "                self.proba_step = act_model.proba_step\n",
    "                self.value = act_model.value\n",
    "                self.initial_state = act_model.initial_state\n",
    "                tf.global_variables_initializer().run(session=self.sess)  # pylint: disable=E1101\n",
    "\n",
    "                self.summary = tf.summary.merge_all()\n",
    "\n",
    "    def _train_step(self, learning_rate, cliprange, obs, returns, masks, actions, values, neglogpacs, update,\n",
    "                    writer, states=None):\n",
    "        \"\"\"\n",
    "        Training of PPO2 Algorithm\n",
    "\n",
    "        :param learning_rate: (float) learning rate\n",
    "        :param cliprange: (float) Clipping factor\n",
    "        :param obs: (np.ndarray) The current observation of the environment\n",
    "        :param returns: (np.ndarray) the rewards\n",
    "        :param masks: (np.ndarray) The last masks for done episodes (used in recurent policies)\n",
    "        :param actions: (np.ndarray) the actions\n",
    "        :param values: (np.ndarray) the values\n",
    "        :param neglogpacs: (np.ndarray) Negative Log-likelihood probability of Actions\n",
    "        :param update: (int) the current step iteration\n",
    "        :param writer: (TensorFlow Summary.writer) the writer for tensorboard\n",
    "        :param states: (np.ndarray) For recurrent policies, the internal state of the recurrent model\n",
    "        :return: policy gradient loss, value function loss, policy entropy,\n",
    "                approximation of kl divergence, updated clipping range, training update operation\n",
    "        \"\"\"\n",
    "        advs = returns - values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        td_map = {self.train_model.obs_ph: obs, self.action_ph: actions, self.advs_ph: advs, self.rewards_ph: returns,\n",
    "                  self.learning_rate_ph: learning_rate, self.clip_range_ph: cliprange,\n",
    "                  self.old_neglog_pac_ph: neglogpacs, self.old_vpred_ph: values}\n",
    "        if states is not None:\n",
    "            td_map[self.train_model.states_ph] = states\n",
    "            td_map[self.train_model.masks_ph] = masks\n",
    "\n",
    "        if states is None:\n",
    "            update_fac = self.n_batch // self.nminibatches // self.noptepochs\n",
    "        else:\n",
    "            update_fac = self.n_batch // self.nminibatches // self.noptepochs // self.n_steps\n",
    "\n",
    "        if writer is not None:\n",
    "            # run loss backprop with summary, but once every 10 runs save the metadata (memory, compute time, ...)\n",
    "            if (1 + update) % 10 == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train],\n",
    "                    td_map, options=run_options, run_metadata=run_metadata)\n",
    "                writer.add_run_metadata(run_metadata, 'step%d' % (update * update_fac))\n",
    "            else:\n",
    "                summary, policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train],\n",
    "                    td_map)\n",
    "            writer.add_summary(summary, (update * update_fac))\n",
    "        else:\n",
    "            policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                [self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train], td_map)\n",
    "\n",
    "        return policy_loss, value_loss, policy_entropy, approxkl, clipfrac\n",
    "    \n",
    "    def learn_setup(self, total_timesteps):\n",
    "        self.runner = Runner(env=self.env, model=self, n_steps=self.n_steps, gamma=self.gamma, lam=self.lam)\n",
    "        self.episode_reward = np.zeros((self.n_envs,))\n",
    "        self.ep_info_buf = deque(maxlen=100)\n",
    "        self.t_first_start = time.time()\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.nupdates = self.total_timesteps // self.n_batch\n",
    "        self.update = 0\n",
    "        \n",
    "        return self.runner\n",
    "    \n",
    "    def learn_setup_runnerless(self, total_timesteps, tb_log_name):\n",
    "        self.episode_reward = np.zeros((self.n_envs,))\n",
    "        self.ep_info_buf = deque(maxlen=100)\n",
    "        self.t_first_start = time.time()\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.nupdates = self.total_timesteps // self.n_batch\n",
    "        self.update = 0\n",
    "        self.tb_log_name = tb_log_name\n",
    "        \n",
    "    def learn_step(self, run, callback = None, sub_num=0):\n",
    "        log_interval = 1\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, self.tb_log_name) as writer:\n",
    "            assert self.n_batch % self.nminibatches == 0\n",
    "            n_batch_train = self.n_batch // self.nminibatches\n",
    "            t_start = time.time()\n",
    "            frac = 1.0 - (self.update - 1.0) / self.nupdates\n",
    "            lr_now = self.learning_rate(frac)\n",
    "            cliprangenow = self.cliprange(frac)\n",
    "            # true_reward is the reward without discount\n",
    "            obs, returns, masks, actions, values, neglogpacs, states, ep_infos, true_reward = run\n",
    "            self.ep_info_buf.extend(ep_infos)\n",
    "            mb_loss_vals = []\n",
    "\n",
    "            inds = np.arange(self.n_batch)\n",
    "            for epoch_num in range(self.noptepochs):\n",
    "                np.random.shuffle(inds)\n",
    "                for start in range(0, self.n_batch, n_batch_train):\n",
    "                    timestep = ((self.update * self.noptepochs * self.n_batch + epoch_num * self.n_batch + start) //\n",
    "                                n_batch_train)\n",
    "                    end = start + n_batch_train\n",
    "                    mbinds = inds[start:end]\n",
    "                    slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "                    mb_loss_vals.append(self._train_step(lr_now, cliprangenow, *slices, writer=writer,\n",
    "                                                         update=timestep))\n",
    "\n",
    "            loss_vals = np.mean(mb_loss_vals, axis=0)\n",
    "            t_now = time.time()\n",
    "            fps = int(self.n_batch / (t_now - t_start))\n",
    "\n",
    "            if writer is not None:\n",
    "                self.episode_reward = total_episode_reward_logger(self.episode_reward,\n",
    "                                                                  true_reward.reshape((self.n_envs, self.n_steps)),\n",
    "                                                                  masks.reshape((self.n_envs, self.n_steps)),\n",
    "                                                                  writer, self.update * (self.n_batch + 1))\n",
    "            if callback is not None:\n",
    "                callback(locals(), globals(), sub_num)\n",
    "\n",
    "            if self.verbose >= 1 and (self.update % log_interval == 0 or self.update == 1):\n",
    "                explained_var = explained_variance(values, returns)\n",
    "                logger.logkv(\"serial_timesteps\", (self.update + 1) * self.n_steps)\n",
    "                logger.logkv(\"nupdates\", (self.update + 1))\n",
    "                logger.logkv(\"total_timesteps\", (self.update + 1) * self.n_batch)\n",
    "                logger.logkv(\"fps\", fps)\n",
    "                logger.logkv(\"explained_variance\", float(explained_var))\n",
    "                logger.logkv('ep_rewmean', safe_mean([ep_info['r'] for ep_info in self.ep_info_buf]))\n",
    "                logger.logkv('eplenmean', safe_mean([ep_info['l'] for ep_info in self.ep_info_buf]))\n",
    "                logger.logkv('time_elapsed', t_start - self.t_first_start)\n",
    "                for (loss_val, loss_name) in zip(loss_vals, self.loss_names):\n",
    "                    logger.logkv(loss_name, loss_val)\n",
    "                logger.dumpkvs()\n",
    "                \n",
    "            self.update += 1\n",
    "            return self\n",
    "        \n",
    "    def learn(self, timesteps):\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def save(self, save_path):\n",
    "        data = {\n",
    "            \"gamma\": self.gamma,\n",
    "            \"n_steps\": self.n_steps,\n",
    "            \"vf_coef\": self.vf_coef,\n",
    "            \"ent_coef\": self.ent_coef,\n",
    "            \"max_grad_norm\": self.max_grad_norm,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"lam\": self.lam,\n",
    "            \"nminibatches\": self.nminibatches,\n",
    "            \"noptepochs\": self.noptepochs,\n",
    "            \"cliprange\": self.cliprange,\n",
    "            \"verbose\": self.verbose,\n",
    "            \"policy\": self.policy,\n",
    "            \"observation_space\": self.observation_space,\n",
    "            \"action_space\": self.action_space,\n",
    "            \"n_envs\": self.n_envs,\n",
    "            \"_vectorize_action\": self._vectorize_action\n",
    "        }\n",
    "\n",
    "        params = self.sess.run(self.params)\n",
    "\n",
    "        self._save_to_file(save_path, data=data, params=params)\n",
    "        \n",
    "def swap_and_flatten(arr):\n",
    "    shape = arr.shape\n",
    "    return arr.swapaxes(0, 1).reshape(shape[0] * shape[1], *shape[2:])\n",
    "\n",
    "def constfn(val):\n",
    "    def func(_):\n",
    "        return val\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, num_obs=2):\n",
    "        self.episode = 0\n",
    "        self.num_obs = num_obs\n",
    "        self.action_space = spaces.MultiBinary(self.num_obs)\n",
    "        self.observation_space = spaces.MultiBinary(self.num_obs)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.episode += 1\n",
    "        self.reward = 0\n",
    "        self.count = 0\n",
    "        self.state = np.random.randint(0,2, (self.num_obs, ))\n",
    "        #self.state = np.array([1])\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0.\n",
    "        for n in range(self.num_obs):\n",
    "            print(\"Env S,A:\", self.state[n], action[n])\n",
    "            if action[n] == self.state[n]:\n",
    "                reward += 1./self.num_obs\n",
    "            else:\n",
    "                reward -= 1./self.num_obs\n",
    "        \n",
    "        print(\"Env R:\", reward)\n",
    "        self.reward += reward\n",
    "            \n",
    "        self.state = np.array(np.random.randint(0,2, (self.num_obs, )))\n",
    "        self.count += 1\n",
    "        done = False\n",
    "        \n",
    "        if self.count > 10:\n",
    "            done = True\n",
    "            \n",
    "        return self.state, reward, done, {'episode': {'r': self.reward, 'l': self.count}}\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "class SubEnvironment:\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.action_space = copy.copy(env.action_space)\n",
    "        self.action_space.shape = (1, )\n",
    "        self.observation_space = env.observation_space\n",
    "    \n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        self.reward = 0.\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def step(self, state, reward, done):\n",
    "        self.reward += reward\n",
    "        self.count += 1\n",
    "        \n",
    "        return state, reward, done, {'episode': {'r': self.reward, 'l': self.count}}\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.spaces.np_random.randint(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /home/ubuntu/ppo_logs/CartPole-v1-33\n",
      "Logging to /tmp/openai-2018-12-02-21-31-06-515923\n",
      "Creating SubModels\n",
      "train step: 0\n",
      "Step: 0\n",
      "r 1.0\n",
      "train step: 1\n",
      "Step: 1\n",
      "r 1.0\n",
      "train step: 2\n",
      "Step: 2\n",
      "r 1.0\n",
      "train step: 3\n",
      "Step: 3\n",
      "r 1.0\n",
      "train step: 4\n",
      "Step: 4\n",
      "r 1.0\n",
      "train step: 5\n",
      "Step: 5\n",
      "r 1.0\n",
      "train step: 6\n",
      "Step: 6\n",
      "r 1.0\n",
      "train step: 7\n",
      "Step: 7\n",
      "r 1.0\n",
      "train step: 8\n",
      "Step: 8\n",
      "r 1.0\n",
      "train step: 9\n",
      "Step: 9\n",
      "r 1.0\n",
      "train step: 10\n",
      "Step: 10\n",
      "r 1.0\n",
      "train step: 11\n",
      "Step: 11\n",
      "r 1.0\n",
      "train step: 12\n",
      "Step: 12\n",
      "r 1.0\n",
      "train step: 13\n",
      "Step: 13\n",
      "r 1.0\n",
      "train step: 14\n",
      "Step: 14\n",
      "r 1.0\n",
      "train step: 15\n",
      "Step: 15\n",
      "r 1.0\n",
      "train step: 16\n",
      "Step: 16\n",
      "r 1.0\n",
      "train step: 17\n",
      "Step: 17\n",
      "r 1.0\n",
      "train step: 18\n",
      "Step: 18\n",
      "r 1.0\n",
      "train step: 19\n",
      "Step: 19\n",
      "r 1.0\n",
      "train step: 20\n",
      "Step: 20\n",
      "r 1.0\n",
      "train step: 21\n",
      "Step: 21\n",
      "r 1.0\n",
      "train step: 22\n",
      "Step: 22\n",
      "r 1.0\n",
      "train step: 23\n",
      "Step: 23\n",
      "r 1.0\n",
      "train step: 24\n",
      "Step: 24\n",
      "r 1.0\n",
      "train step: 25\n",
      "Step: 25\n",
      "r 1.0\n",
      "train step: 26\n",
      "Step: 26\n",
      "r 1.0\n",
      "train step: 27\n",
      "Step: 27\n",
      "r 1.0\n",
      "train step: 28\n",
      "Step: 28\n",
      "r 1.0\n",
      "train step: 29\n",
      "Step: 29\n",
      "r 1.0\n",
      "train step: 30\n",
      "Step: 30\n",
      "r 1.0\n",
      "train step: 31\n",
      "Step: 31\n",
      "r 1.0\n",
      "train step: 32\n",
      "Step: 32\n",
      "r 1.0\n",
      "train step: 33\n",
      "Step: 33\n",
      "r 1.0\n",
      "train step: 34\n",
      "Step: 34\n",
      "r 1.0\n",
      "train step: 35\n",
      "Step: 35\n",
      "r 1.0\n",
      "train step: 36\n",
      "Step: 36\n",
      "r 1.0\n",
      "train step: 37\n",
      "Step: 37\n",
      "r 1.0\n",
      "train step: 38\n",
      "Step: 38\n",
      "r 1.0\n",
      "train step: 39\n",
      "Step: 39\n",
      "r 1.0\n",
      "train step: 40\n",
      "Step: 40\n",
      "r 1.0\n",
      "train step: 41\n",
      "Step: 41\n",
      "r 1.0\n",
      "train step: 42\n",
      "Step: 42\n",
      "r 1.0\n",
      "train step: 43\n",
      "Step: 43\n",
      "r 1.0\n",
      "train step: 44\n",
      "Step: 44\n",
      "r 1.0\n",
      "train step: 45\n",
      "Step: 45\n",
      "r 1.0\n",
      "train step: 46\n",
      "Step: 46\n",
      "r 1.0\n",
      "train step: 47\n",
      "Step: 47\n",
      "r 1.0\n",
      "train step: 48\n",
      "Step: 48\n",
      "r 1.0\n",
      "train step: 49\n",
      "Step: 49\n",
      "r 1.0\n",
      "train step: 50\n",
      "Step: 50\n",
      "r 1.0\n",
      "train step: 51\n",
      "Step: 51\n",
      "r 1.0\n",
      "train step: 52\n",
      "Step: 52\n",
      "r 1.0\n",
      "train step: 53\n",
      "Step: 53\n",
      "r 1.0\n",
      "train step: 54\n",
      "Step: 54\n",
      "r 1.0\n",
      "train step: 55\n",
      "Step: 55\n",
      "r 1.0\n",
      "train step: 56\n",
      "Step: 56\n",
      "r 1.0\n",
      "train step: 57\n",
      "Step: 57\n",
      "r 1.0\n",
      "train step: 58\n",
      "Step: 58\n",
      "r 1.0\n",
      "train step: 59\n",
      "Step: 59\n",
      "r 1.0\n",
      "train step: 60\n",
      "Step: 60\n",
      "r 1.0\n",
      "train step: 61\n",
      "Step: 61\n",
      "r 1.0\n",
      "train step: 62\n",
      "Step: 62\n",
      "r 1.0\n",
      "train step: 63\n",
      "Step: 63\n",
      "r 1.0\n",
      "train step: 64\n",
      "Step: 64\n",
      "r 1.0\n",
      "train step: 65\n",
      "Step: 65\n",
      "r 1.0\n",
      "train step: 66\n",
      "Step: 66\n",
      "r 1.0\n",
      "train step: 67\n",
      "Step: 67\n",
      "r 1.0\n",
      "train step: 68\n",
      "Step: 68\n",
      "r 1.0\n",
      "train step: 69\n",
      "Step: 69\n",
      "r 1.0\n",
      "train step: 70\n",
      "Step: 70\n",
      "r 1.0\n",
      "train step: 71\n",
      "Step: 71\n",
      "r 1.0\n",
      "train step: 72\n",
      "Step: 72\n",
      "r 1.0\n",
      "train step: 73\n",
      "Step: 73\n",
      "r 1.0\n",
      "train step: 74\n",
      "Step: 74\n",
      "r 1.0\n",
      "train step: 75\n",
      "Step: 75\n",
      "r 1.0\n",
      "train step: 76\n",
      "Step: 76\n",
      "r 1.0\n",
      "train step: 77\n",
      "Step: 77\n",
      "r 1.0\n",
      "train step: 78\n",
      "Step: 78\n",
      "r 1.0\n",
      "train step: 79\n",
      "Step: 79\n",
      "r 1.0\n",
      "train step: 80\n",
      "Step: 80\n",
      "r 1.0\n",
      "train step: 81\n",
      "Step: 81\n",
      "r 1.0\n",
      "train step: 82\n",
      "Step: 82\n",
      "r 1.0\n",
      "train step: 83\n",
      "Step: 83\n",
      "r 1.0\n",
      "train step: 84\n",
      "Step: 84\n",
      "r 1.0\n",
      "train step: 85\n",
      "Step: 85\n",
      "r 1.0\n",
      "train step: 86\n",
      "Step: 86\n",
      "r 1.0\n",
      "train step: 87\n",
      "Step: 87\n",
      "r 1.0\n",
      "train step: 88\n",
      "Step: 88\n",
      "r 1.0\n",
      "train step: 89\n",
      "Step: 89\n",
      "r 1.0\n",
      "train step: 90\n",
      "Step: 90\n",
      "r 1.0\n",
      "train step: 91\n",
      "Step: 91\n",
      "r 1.0\n",
      "train step: 92\n",
      "Step: 92\n",
      "r 1.0\n",
      "train step: 93\n",
      "Step: 93\n",
      "r 1.0\n",
      "train step: 94\n",
      "Step: 94\n",
      "r 1.0\n",
      "train step: 95\n",
      "Step: 95\n",
      "r 1.0\n",
      "train step: 96\n",
      "Step: 96\n",
      "r 1.0\n",
      "train step: 97\n",
      "Step: 97\n",
      "r 1.0\n",
      "train step: 98\n",
      "Step: 98\n",
      "r 1.0\n",
      "train step: 99\n",
      "Step: 99\n",
      "6221 timesteps\n",
      "Best mean reward: -inf - Last mean reward per episode: 56.65\n",
      "Saving new best model\n",
      "r 1.0\n",
      "train step: 100\n",
      "Step: 100\n",
      "r 1.0\n",
      "train step: 101\n",
      "Step: 101\n",
      "r 1.0\n",
      "train step: 102\n",
      "Step: 102\n",
      "r 1.0\n",
      "train step: 103\n",
      "Step: 103\n",
      "r 1.0\n",
      "train step: 104\n",
      "Step: 104\n",
      "r 1.0\n",
      "train step: 105\n",
      "Step: 105\n",
      "r 1.0\n",
      "train step: 106\n",
      "Step: 106\n",
      "r 1.0\n",
      "train step: 107\n",
      "Step: 107\n",
      "r 1.0\n",
      "train step: 108\n",
      "Step: 108\n",
      "r 1.0\n",
      "train step: 109\n",
      "Step: 109\n",
      "r 1.0\n",
      "train step: 110\n",
      "Step: 110\n",
      "r 1.0\n",
      "train step: 111\n",
      "Step: 111\n",
      "r 1.0\n",
      "train step: 112\n",
      "Step: 112\n",
      "r 1.0\n",
      "train step: 113\n",
      "Step: 113\n",
      "r 1.0\n",
      "train step: 114\n",
      "Step: 114\n",
      "r 1.0\n",
      "train step: 115\n",
      "Step: 115\n",
      "r 1.0\n",
      "train step: 116\n",
      "Step: 116\n",
      "r 1.0\n",
      "train step: 117\n",
      "Step: 117\n",
      "r 1.0\n",
      "train step: 118\n",
      "Step: 118\n",
      "r 1.0\n",
      "train step: 119\n",
      "Step: 119\n",
      "r 1.0\n",
      "train step: 120\n",
      "Step: 120\n",
      "r 1.0\n",
      "train step: 121\n",
      "Step: 121\n",
      "r 1.0\n",
      "train step: 122\n",
      "Step: 122\n",
      "r 1.0\n",
      "train step: 123\n",
      "Step: 123\n",
      "r 1.0\n",
      "train step: 124\n",
      "Step: 124\n",
      "r 1.0\n",
      "train step: 125\n",
      "Step: 125\n",
      "r 1.0\n",
      "train step: 126\n",
      "Step: 126\n",
      "r 1.0\n",
      "train step: 127\n",
      "Step: 127\n",
      "r 1.0\n",
      "train step: 128\n",
      "Step: 128\n",
      "r 1.0\n",
      "train step: 129\n",
      "Step: 129\n",
      "r 1.0\n",
      "train step: 130\n",
      "Step: 130\n",
      "r 1.0\n",
      "train step: 131\n",
      "Step: 131\n",
      "r 1.0\n",
      "train step: 132\n",
      "Step: 132\n",
      "r 1.0\n",
      "train step: 133\n",
      "Step: 133\n",
      "r 1.0\n",
      "train step: 134\n",
      "Step: 134\n",
      "r 1.0\n",
      "train step: 135\n",
      "Step: 135\n",
      "r 1.0\n",
      "train step: 136\n",
      "Step: 136\n",
      "r 1.0\n",
      "train step: 137\n",
      "Step: 137\n",
      "r 1.0\n",
      "train step: 138\n",
      "Step: 138\n",
      "r 1.0\n",
      "train step: 139\n",
      "Step: 139\n",
      "r 1.0\n",
      "train step: 140\n",
      "Step: 140\n",
      "r 1.0\n",
      "train step: 141\n",
      "Step: 141\n",
      "r 1.0\n",
      "train step: 142\n",
      "Step: 142\n",
      "r 1.0\n",
      "train step: 143\n",
      "Step: 143\n",
      "r 1.0\n",
      "train step: 144\n",
      "Step: 144\n",
      "r 1.0\n",
      "train step: 145\n",
      "Step: 145\n",
      "r 1.0\n",
      "train step: 146\n",
      "Step: 146\n",
      "r 1.0\n",
      "train step: 147\n",
      "Step: 147\n",
      "r 1.0\n",
      "train step: 148\n",
      "Step: 148\n",
      "r 1.0\n",
      "train step: 149\n",
      "Step: 149\n",
      "r 1.0\n",
      "train step: 150\n",
      "Step: 150\n",
      "r 1.0\n",
      "train step: 151\n",
      "Step: 151\n",
      "r 1.0\n",
      "train step: 152\n",
      "Step: 152\n",
      "r 1.0\n",
      "train step: 153\n",
      "Step: 153\n",
      "r 1.0\n",
      "train step: 154\n",
      "Step: 154\n",
      "r 1.0\n",
      "train step: 155\n",
      "Step: 155\n",
      "r 1.0\n",
      "train step: 156\n",
      "Step: 156\n",
      "r 1.0\n",
      "train step: 157\n",
      "Step: 157\n",
      "r 1.0\n",
      "train step: 158\n",
      "Step: 158\n",
      "r 1.0\n",
      "train step: 159\n",
      "Step: 159\n",
      "r 1.0\n",
      "train step: 160\n",
      "Step: 160\n",
      "r 1.0\n",
      "train step: 161\n",
      "Step: 161\n",
      "r 1.0\n",
      "train step: 162\n",
      "Step: 162\n",
      "r 1.0\n",
      "train step: 163\n",
      "Step: 163\n",
      "r 1.0\n",
      "train step: 164\n",
      "Step: 164\n",
      "r 1.0\n",
      "train step: 165\n",
      "Step: 165\n",
      "r 1.0\n",
      "train step: 166\n",
      "Step: 166\n",
      "r 1.0\n",
      "train step: 167\n",
      "Step: 167\n",
      "r 1.0\n",
      "train step: 168\n",
      "Step: 168\n",
      "r 1.0\n",
      "train step: 169\n",
      "Step: 169\n",
      "r 1.0\n",
      "train step: 170\n",
      "Step: 170\n",
      "r 1.0\n",
      "train step: 171\n",
      "Step: 171\n",
      "r 1.0\n",
      "train step: 172\n",
      "Step: 172\n",
      "r 1.0\n",
      "train step: 173\n",
      "Step: 173\n",
      "r 1.0\n",
      "train step: 174\n",
      "Step: 174\n",
      "r 1.0\n",
      "train step: 175\n",
      "Step: 175\n",
      "r 1.0\n",
      "train step: 176\n",
      "Step: 176\n",
      "r 1.0\n",
      "train step: 177\n",
      "Step: 177\n",
      "r 1.0\n",
      "train step: 178\n",
      "Step: 178\n",
      "r 1.0\n",
      "train step: 179\n",
      "Step: 179\n",
      "r 1.0\n",
      "train step: 180\n",
      "Step: 180\n",
      "r 1.0\n",
      "train step: 181\n",
      "Step: 181\n",
      "r 1.0\n",
      "train step: 182\n",
      "Step: 182\n",
      "r 1.0\n",
      "train step: 183\n",
      "Step: 183\n",
      "r 1.0\n",
      "train step: 184\n",
      "Step: 184\n",
      "r 1.0\n",
      "train step: 185\n",
      "Step: 185\n",
      "r 1.0\n",
      "train step: 186\n",
      "Step: 186\n",
      "r 1.0\n",
      "train step: 187\n",
      "Step: 187\n",
      "r 1.0\n",
      "train step: 188\n",
      "Step: 188\n",
      "r 1.0\n",
      "train step: 189\n",
      "Step: 189\n",
      "r 1.0\n",
      "train step: 190\n",
      "Step: 190\n",
      "r 1.0\n",
      "train step: 191\n",
      "Step: 191\n",
      "r 1.0\n",
      "train step: 192\n",
      "Step: 192\n",
      "r 1.0\n",
      "train step: 193\n",
      "Step: 193\n",
      "r 1.0\n",
      "train step: 194\n",
      "Step: 194\n",
      "r 1.0\n",
      "train step: 195\n",
      "Step: 195\n",
      "r 1.0\n",
      "train step: 196\n",
      "Step: 196\n",
      "r 1.0\n",
      "train step: 197\n",
      "Step: 197\n",
      "r 1.0\n",
      "train step: 198\n",
      "Step: 198\n",
      "r 1.0\n",
      "train step: 199\n",
      "Step: 199\n",
      "10115 timesteps\n",
      "Best mean reward: 56.65 - Last mean reward per episode: 79.26\n",
      "Saving new best model\n",
      "r 1.0\n",
      "train step: 200\n",
      "Step: 200\n",
      "r 1.0\n",
      "train step: 201\n",
      "Step: 201\n",
      "r 1.0\n",
      "train step: 202\n",
      "Step: 202\n",
      "r 1.0\n",
      "train step: 203\n",
      "Step: 203\n",
      "r 1.0\n",
      "train step: 204\n",
      "Step: 204\n",
      "r 1.0\n",
      "train step: 205\n",
      "Step: 205\n",
      "r 1.0\n",
      "train step: 206\n",
      "Step: 206\n",
      "r 1.0\n",
      "train step: 207\n",
      "Step: 207\n",
      "r 1.0\n",
      "train step: 208\n",
      "Step: 208\n",
      "r 1.0\n",
      "train step: 209\n",
      "Step: 209\n",
      "r 1.0\n",
      "train step: 210\n",
      "Step: 210\n",
      "r 1.0\n",
      "train step: 211\n",
      "Step: 211\n",
      "r 1.0\n",
      "train step: 212\n",
      "Step: 212\n",
      "r 1.0\n",
      "train step: 213\n",
      "Step: 213\n",
      "r 1.0\n",
      "train step: 214\n",
      "Step: 214\n",
      "r 1.0\n",
      "train step: 215\n",
      "Step: 215\n",
      "r 1.0\n",
      "train step: 216\n",
      "Step: 216\n",
      "r 1.0\n",
      "train step: 217\n",
      "Step: 217\n",
      "r 1.0\n",
      "train step: 218\n",
      "Step: 218\n",
      "r 1.0\n",
      "train step: 219\n",
      "Step: 219\n",
      "r 1.0\n",
      "train step: 220\n",
      "Step: 220\n",
      "r 1.0\n",
      "train step: 221\n",
      "Step: 221\n",
      "r 1.0\n",
      "train step: 222\n",
      "Step: 222\n",
      "r 1.0\n",
      "train step: 223\n",
      "Step: 223\n",
      "r 1.0\n",
      "train step: 224\n",
      "Step: 224\n",
      "r 1.0\n",
      "train step: 225\n",
      "Step: 225\n",
      "r 1.0\n",
      "train step: 226\n",
      "Step: 226\n",
      "r 1.0\n",
      "train step: 227\n",
      "Step: 227\n",
      "r 1.0\n",
      "train step: 228\n",
      "Step: 228\n",
      "r 1.0\n",
      "train step: 229\n",
      "Step: 229\n",
      "r 1.0\n",
      "train step: 230\n",
      "Step: 230\n",
      "r 1.0\n",
      "train step: 231\n",
      "Step: 231\n",
      "r 1.0\n",
      "train step: 232\n",
      "Step: 232\n",
      "r 1.0\n",
      "train step: 233\n",
      "Step: 233\n",
      "r 1.0\n",
      "train step: 234\n",
      "Step: 234\n",
      "r 1.0\n",
      "train step: 235\n",
      "Step: 235\n",
      "r 1.0\n",
      "train step: 236\n",
      "Step: 236\n",
      "r 1.0\n",
      "train step: 237\n",
      "Step: 237\n",
      "r 1.0\n",
      "train step: 238\n",
      "Step: 238\n",
      "r 1.0\n",
      "train step: 239\n",
      "Step: 239\n",
      "r 1.0\n",
      "train step: 240\n",
      "Step: 240\n",
      "r 1.0\n",
      "train step: 241\n",
      "Step: 241\n",
      "r 1.0\n",
      "train step: 242\n",
      "Step: 242\n",
      "r 1.0\n",
      "train step: 243\n",
      "Step: 243\n",
      "r 1.0\n",
      "train step: 244\n",
      "Step: 244\n",
      "r 1.0\n",
      "train step: 245\n",
      "Step: 245\n",
      "r 1.0\n",
      "train step: 246\n",
      "Step: 246\n",
      "r 1.0\n",
      "train step: 247\n",
      "Step: 247\n",
      "r 1.0\n",
      "train step: 248\n",
      "Step: 248\n",
      "r 1.0\n",
      "train step: 249\n",
      "Step: 249\n",
      "r 1.0\n",
      "train step: 250\n",
      "Step: 250\n",
      "r 1.0\n",
      "train step: 251\n",
      "Step: 251\n",
      "r 1.0\n",
      "train step: 252\n",
      "Step: 252\n",
      "r 1.0\n",
      "train step: 253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 253\n",
      "r 1.0\n",
      "train step: 254\n",
      "Step: 254\n",
      "r 1.0\n",
      "train step: 255\n",
      "Step: 255\n",
      "r 1.0\n",
      "train step: 256\n",
      "Step: 256\n",
      "r 1.0\n",
      "train step: 257\n",
      "Step: 257\n",
      "r 1.0\n",
      "train step: 258\n",
      "Step: 258\n",
      "r 1.0\n",
      "train step: 259\n",
      "Step: 259\n",
      "r 1.0\n",
      "train step: 260\n",
      "Step: 260\n",
      "r 1.0\n",
      "train step: 261\n",
      "Step: 261\n",
      "r 1.0\n",
      "train step: 262\n",
      "Step: 262\n",
      "r 1.0\n",
      "train step: 263\n",
      "Step: 263\n",
      "r 1.0\n",
      "train step: 264\n",
      "Step: 264\n",
      "r 1.0\n",
      "train step: 265\n",
      "Step: 265\n",
      "r 1.0\n",
      "train step: 266\n",
      "Step: 266\n",
      "r 1.0\n",
      "train step: 267\n",
      "Step: 267\n",
      "r 1.0\n",
      "train step: 268\n",
      "Step: 268\n",
      "r 1.0\n",
      "train step: 269\n",
      "Step: 269\n",
      "r 1.0\n",
      "train step: 270\n",
      "Step: 270\n",
      "r 1.0\n",
      "train step: 271\n",
      "Step: 271\n",
      "r 1.0\n",
      "train step: 272\n",
      "Step: 272\n",
      "r 1.0\n",
      "train step: 273\n",
      "Step: 273\n",
      "r 1.0\n",
      "train step: 274\n",
      "Step: 274\n",
      "r 1.0\n",
      "train step: 275\n",
      "Step: 275\n",
      "r 1.0\n",
      "train step: 276\n",
      "Step: 276\n",
      "r 1.0\n",
      "train step: 277\n",
      "Step: 277\n",
      "r 1.0\n",
      "train step: 278\n",
      "Step: 278\n",
      "r 1.0\n",
      "train step: 279\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0b7285f78189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# make an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# append data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/stable_baselines/common/policies.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, obs, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             action, value, neglogp = self.sess.run([self.action, self._value, self.neglogp],\n\u001b[0;32m--> 283\u001b[0;31m                                                    {self.obs_ph: obs})\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env_id='CartPole-v1'\n",
    "num_timesteps=10000000\n",
    "seed=343\n",
    "best_mean_reward, n_log_steps = -np.inf, 0\n",
    "\n",
    "base_dir = home + '/ppo_logs'\n",
    "prev = [f for f in os.listdir(base_dir) if env_id in f]\n",
    "log_dir = base_dir + '/{}-{}'.format(env_id, len(prev))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print('Logging to {}'.format(log_dir))\n",
    "\n",
    "logger.configure()\n",
    "\n",
    "def make_env(_env_id):\n",
    "    env_out = gym.make(_env_id)\n",
    "    env_out = bench.Monitor(env_out, log_dir, allow_early_resets=True)\n",
    "    return env_out\n",
    "\n",
    "env = DummyVecEnv([lambda: make_env(env_id)])\n",
    "#env = VecNormalize(env)\n",
    "\n",
    "def callback(_locals, _globals, sub_num):\n",
    "    global n_log_steps, best_mean_reward\n",
    "    print(\"Step:\", n_log_steps)\n",
    "\n",
    "    if (n_log_steps + 1) % 100 == 0:\n",
    "        x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            mean_reward = np.mean(y[-100:])\n",
    "            print(x[-1], 'timesteps')\n",
    "            print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(best_mean_reward, mean_reward))\n",
    "\n",
    "            if mean_reward > best_mean_reward:\n",
    "                best_mean_reward = mean_reward\n",
    "                # Example for saving best model\n",
    "                print(\"Saving new best model\")\n",
    "                _locals['self'].save(\"model_{}_{}\".format(env_id, sub_num))\n",
    "    n_log_steps += 1\n",
    "    return False\n",
    "\n",
    "set_global_seeds(seed)\n",
    "# policy = MlpPolicy\n",
    "# model = PPO2(policy=policy, env=env, n_steps=2048, nminibatches=1, lam=0.95, gamma=0.99, noptepochs=10,\n",
    "#              ent_coef=0.0, learning_rate=3e-4, cliprange=0.2, verbose=1, tensorboard_log=log_dir)\n",
    "# model.learn(total_timesteps=num_timesteps, callback=callback)\n",
    "# model.save(\"model_{}\".format(env_id))\n",
    "\n",
    "# return model, env\n",
    "\n",
    "submodels = []\n",
    "num_subs = 1#env.action_space.n#shape[0]\n",
    "\n",
    "print(\"Creating SubModels\")\n",
    "for _ in range(num_subs):\n",
    "    sm = PPO2_Mod(MlpPolicy, DummyVecEnv([lambda: SubEnvironment(env)]), verbose=0)\n",
    "    sm.learn_setup_runnerless(num_timesteps, log_dir + '_{}'.format(_))\n",
    "    submodels.append(sm)\n",
    "    \n",
    "#env = environment(num_subs)\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "\n",
    "n_steps = 128\n",
    "average_rewards = []\n",
    "for train_step in range(num_timesteps):\n",
    "    print(\"train step:\", train_step)\n",
    "    \n",
    "    ###\n",
    "    # create a run\n",
    "    ###\n",
    "    \n",
    "    # minibatches\n",
    "    mbs = []\n",
    "    for n in range(num_subs):\n",
    "        mbs.append({\n",
    "            'obs': [],\n",
    "            'rewards': [],\n",
    "            'actions': [],\n",
    "            'values': [],\n",
    "            'dones': [],\n",
    "            'neglogpacs': [],\n",
    "            'ep_infos': []\n",
    "        })\n",
    "    \n",
    "    mb_states = None\n",
    "    ep_infos = []\n",
    "    dones = False\n",
    "    train_step_rewards = []\n",
    "    \n",
    "    obs = np.zeros((1,) + env.observation_space.shape, dtype=env.observation_space.dtype.name)\n",
    "    obs[:] = env.reset()\n",
    "    \n",
    "    # for each step\n",
    "    for mb_step in range(n_steps):\n",
    "        augmented_actions = []\n",
    "        # for each submodel\n",
    "        for n, submodel in enumerate(submodels):\n",
    "            # make an action\n",
    "            actions, values, states, neglogpacs = submodel.step(obs)\n",
    "            \n",
    "            # append data\n",
    "            mbs[n]['obs'].append(obs.copy())\n",
    "            mbs[n]['actions'].append(actions)\n",
    "            mbs[n]['values'].append(values)\n",
    "            mbs[n]['neglogpacs'].append(neglogpacs)\n",
    "            mbs[n]['dones'].append([dones])\n",
    "            \n",
    "            # collection the actions\n",
    "            clipped_actions = actions\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                clipped_actions = np.clip(actions, env.action_space.low[n], env.action_space.high[n])\n",
    "            augmented_actions.append(clipped_actions)\n",
    "        \n",
    "        # combine the actions\n",
    "        #augmented_actions = [np.array(augmented_actions).reshape((num_subs, ))]\n",
    "        \n",
    "        # step in the environment\n",
    "        obs[:], rewards, dones, infos = env.step(augmented_actions[0])\n",
    "        \n",
    "        train_step_rewards.append(rewards)\n",
    "        \n",
    "        # for each submodel\n",
    "        for n, submodel in enumerate(submodels):\n",
    "            # append info\n",
    "            for info in infos:\n",
    "                maybeep_info = info.get('episode')\n",
    "                if maybeep_info:\n",
    "                    mbs[n]['ep_infos'].append(maybeep_info)\n",
    "            # append reward\n",
    "            mbs[n]['rewards'].append(rewards)\n",
    "            \n",
    "    # 413 ppo2.py\n",
    "    for n, submodel in enumerate(submodels):\n",
    "        mb_obs = np.asarray(mbs[n]['obs'], dtype=obs.dtype)\n",
    "        mb_rewards = np.asarray(mbs[n]['rewards'], dtype=np.float32)\n",
    "        mb_actions = np.asarray(mbs[n]['actions'])\n",
    "        mb_values = np.asarray(mbs[n]['values'], dtype=np.float32)\n",
    "        mb_neglogpacs = np.asarray(mbs[n]['neglogpacs'], dtype=np.float32)\n",
    "        mb_dones = np.asarray(mbs[n]['dones'], dtype=np.bool)\n",
    "        last_values = submodel.value(obs, None, dones)\n",
    "        \n",
    "        mb_advs = np.zeros_like(mb_rewards)\n",
    "        true_reward = np.copy(mb_rewards)\n",
    "        last_gae_lam = 0\n",
    "        \n",
    "        for step in reversed(range(n_steps)):\n",
    "            if step == n_steps - 1:\n",
    "                nextnonterminal = 1.0 - dones\n",
    "                nextvalues = last_values\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - mb_dones[step + 1]\n",
    "                nextvalues = mb_values[step + 1]\n",
    "            delta = mb_rewards[step] + gamma * nextvalues * nextnonterminal - mb_values[step]\n",
    "            mb_advs[step] = last_gae_lam = delta + gamma * lam * nextnonterminal * last_gae_lam\n",
    "        mb_returns = mb_advs + mb_values\n",
    "        \n",
    "        mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs = \\\n",
    "            map(swap_and_flatten, (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs))\n",
    "\n",
    "        run = mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs, mb_states, ep_infos, true_reward\n",
    "        submodel.learn_step(run, callback, n)\n",
    "        \n",
    "    average_rewards.append(np.mean(train_step_rewards))\n",
    "    print(\"r\", np.mean(train_step_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test submodels\n",
    "env = gym.make(ENV_NAME)\n",
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "for i in range(100):\n",
    "    action = []\n",
    "    for sm in submodels:\n",
    "        action.append(sm.predict(obs, deterministic=True)[0])\n",
    "    action = np.array(action).reshape(num_subs,)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    total_reward += rewards\n",
    "    \n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(average_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "--------------------------------------\n",
      "| approxkl           | 9.4723895e-05 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_rewmean         | nan           |\n",
      "| eplenmean          | nan           |\n",
      "| explained_variance | -0.000188     |\n",
      "| fps                | 191           |\n",
      "| nupdates           | 1             |\n",
      "| policy_entropy     | 2.836264      |\n",
      "| policy_loss        | -0.0014905145 |\n",
      "| serial_timesteps   | 128           |\n",
      "| time_elapsed       | 4.53e-06      |\n",
      "| total_timesteps    | 128           |\n",
      "| value_loss         | 724.592       |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x7fe8ed2b3c88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "model.learn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "-11.344475038536892\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "env = gym.make(ENV_NAME)\n",
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    total_reward += rewards\n",
    "    \n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
