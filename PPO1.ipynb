{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from stable_baselines.common.cmd_util import mujoco_arg_parser\n",
    "from stable_baselines import bench, logger\n",
    "from stable_baselines.common import set_global_seeds\n",
    "from stable_baselines.common.vec_env.vec_normalize import VecNormalize\n",
    "from stable_baselines.ppo1 import PPO1\n",
    "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, CnnPolicy\n",
    "from stable_baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from stable_baselines.results_plotter import load_results, ts2xy\n",
    "from stable_baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "\n",
    "home = str(Path.home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from mpi4py import MPI\n",
    "\n",
    "from stable_baselines.common import Dataset, explained_variance, fmt_row, zipsame, ActorCriticRLModel, SetVerbosity, \\\n",
    "    TensorboardWriter\n",
    "import stable_baselines.common.tf_util as tf_util\n",
    "from stable_baselines.common.policies import LstmPolicy, ActorCriticPolicy\n",
    "from stable_baselines.common.mpi_adam import MpiAdam\n",
    "from stable_baselines.common.mpi_moments import mpi_moments\n",
    "from stable_baselines.trpo_mpi.utils import add_vtarg_and_adv, flatten_lists\n",
    "from stable_baselines.a2c.utils import total_episode_reward_logger\n",
    "\n",
    "from stable_baselines.common.vec_env import VecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_segment_generator(policy, env, horizon, reward_giver=None, gail=False):\n",
    "    \"\"\"\n",
    "    Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)\n",
    "    :param policy: (MLPPolicy) the policy\n",
    "    :param env: (Gym Environment) the environment\n",
    "    :param horizon: (int) the number of timesteps to run per batch\n",
    "    :param reward_giver: (TransitionClassifier) the reward predicter from obsevation and action\n",
    "    :param gail: (bool) Whether we are using this generator for standard trpo or with gail\n",
    "    :return: (dict) generator that returns a dict with the following keys:\n",
    "        - ob: (np.ndarray) observations\n",
    "        - rew: (numpy float) rewards (if gail is used it is the predicted reward)\n",
    "        - vpred: (numpy float) action logits\n",
    "        - new: (numpy bool) dones (is end of episode)\n",
    "        - ac: (np.ndarray) actions\n",
    "        - prevac: (np.ndarray) previous actions\n",
    "        - nextvpred: (numpy float) next action logits\n",
    "        - ep_rets: (float) cumulated current episode reward\n",
    "        - ep_lens: (int) the length of the current episode\n",
    "        - ep_true_rets: (float) the real environment reward\n",
    "    \"\"\"\n",
    "    # Check when using GAIL\n",
    "    assert not (gail and reward_giver is None), \"You must pass a reward giver when using GAIL\"\n",
    "\n",
    "    # Initialize state variables\n",
    "    step = 0\n",
    "    action = env.action_space.sample()  # not used, just so we have the datatype\n",
    "    new = True\n",
    "    observation = env.reset()\n",
    "\n",
    "    cur_ep_ret = 0  # return in current episode\n",
    "    cur_ep_len = 0  # len of current episode\n",
    "    cur_ep_true_ret = 0\n",
    "    ep_true_rets = []\n",
    "    ep_rets = []  # returns of completed episodes in this segment\n",
    "    ep_lens = []  # Episode lengths\n",
    "\n",
    "    # Initialize history arrays\n",
    "    observations = np.array([observation for _ in range(horizon)])\n",
    "    true_rews = np.zeros(horizon, 'float32')\n",
    "    rews = np.zeros(horizon, 'float32')\n",
    "    vpreds = np.zeros(horizon, 'float32')\n",
    "    dones = np.zeros(horizon, 'int32')\n",
    "    actions = np.array([action for _ in range(horizon)])\n",
    "    prev_actions = actions.copy()\n",
    "    states = policy.initial_state\n",
    "    done = None\n",
    "\n",
    "    action_history = []\n",
    "    prevac = action\n",
    "    while True:\n",
    "        action_history.append(action)\n",
    "        prevprevac = prevac\n",
    "        prevac = action\n",
    "        action, vpred, states, _ = policy.step(observation.reshape(-1, *observation.shape), states, done)\n",
    "        # Slight weirdness here because we need value function at time T\n",
    "        # before returning segment [0, T-1] so we get the correct\n",
    "        # terminal value\n",
    "        if step > 0 and step % horizon == 0:\n",
    "            # Fix to avoid \"mean of empty slice\" warning when there is only one episode\n",
    "            if len(ep_rets) == 0:\n",
    "                ep_rets = [cur_ep_ret]\n",
    "                ep_lens = [cur_ep_len]\n",
    "                ep_true_rets = [cur_ep_true_ret]\n",
    "                total_timesteps = cur_ep_len\n",
    "            else:\n",
    "                total_timesteps = sum(ep_lens) + cur_ep_len\n",
    "\n",
    "            yield {\"ob\": observations, \"rew\": rews, \"dones\": dones, \"true_rew\": true_rews, \"vpred\": vpreds,\n",
    "                   \"ac\": actions, \"prevac\": prev_actions, \"nextvpred\": vpred * (1 - new), \"ep_rets\": ep_rets,\n",
    "                   \"ep_lens\": ep_lens, \"ep_true_rets\": ep_true_rets, \"total_timestep\": total_timesteps}\n",
    "            _, vpred, _, _ = policy.step(observation.reshape(-1, *observation.shape))\n",
    "            # Be careful!!! if you change the downstream algorithm to aggregate\n",
    "            # several of these batches, then be sure to do a deepcopy\n",
    "            ep_rets = []\n",
    "            ep_true_rets = []\n",
    "            ep_lens = []\n",
    "        i = step % horizon\n",
    "        observations[i] = observation\n",
    "        vpreds[i] = vpred[0]\n",
    "        actions[i] = action[0]\n",
    "        prev_actions[i] = prevac\n",
    "\n",
    "        clipped_action = action\n",
    "        # Clip the actions to avoid out of bound error\n",
    "        if isinstance(env.action_space, gym.spaces.Box):\n",
    "            clipped_action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "\n",
    "        if gail:\n",
    "            rew = reward_giver.get_reward(observation, clipped_action[0])\n",
    "            observation, true_rew, done, _info = env.step(clipped_action[0])\n",
    "        else:\n",
    "            observation, rew, done, _info = env.step(clipped_action[0])\n",
    "            true_rew = rew\n",
    "            \n",
    "        \"\"\"*************\"\"\"\n",
    "        if prevprevac == 2 and prevac == 2 and action == 3:\n",
    "            rew += 0.25\n",
    "        if prevprevac == 3 and prevac == 3 and action == 2:\n",
    "            rew += 0.25\n",
    "        \n",
    "        \"\"\"*************\"\"\"\n",
    "        \n",
    "        rews[i] = rew\n",
    "        true_rews[i] = true_rew\n",
    "        dones[i] = done\n",
    "\n",
    "        cur_ep_ret += rew\n",
    "        cur_ep_true_ret += true_rew\n",
    "        cur_ep_len += 1\n",
    "        if done:\n",
    "            ep_rets.append(cur_ep_ret)\n",
    "            ep_true_rets.append(cur_ep_true_ret)\n",
    "            ep_lens.append(cur_ep_len)\n",
    "            cur_ep_ret = 0\n",
    "            cur_ep_true_ret = 0\n",
    "            cur_ep_len = 0\n",
    "            if not isinstance(env, VecEnv):\n",
    "                observation = env.reset()\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO1_Mod(ActorCriticRLModel):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (MPI version).\n",
    "    Paper: https://arxiv.org/abs/1707.06347\n",
    "    :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n",
    "    :param policy: (ActorCriticPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, CnnLstmPolicy, ...)\n",
    "    :param timesteps_per_actorbatch: (int) timesteps per actor per update\n",
    "    :param clip_param: (float) clipping parameter epsilon\n",
    "    :param entcoeff: (float) the entropy loss weight\n",
    "    :param optim_epochs: (float) the optimizer's number of epochs\n",
    "    :param optim_stepsize: (float) the optimizer's stepsize\n",
    "    :param optim_batchsize: (int) the optimizer's the batch size\n",
    "    :param gamma: (float) discount factor\n",
    "    :param lam: (float) advantage estimation\n",
    "    :param adam_epsilon: (float) the epsilon value for the adam optimizer\n",
    "    :param schedule: (str) The type of scheduler for the learning rate update ('linear', 'constant',\n",
    "        'double_linear_con', 'middle_drop' or 'double_middle_drop')\n",
    "    :param verbose: (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n",
    "    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n",
    "    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, policy, env, gamma=0.99, timesteps_per_actorbatch=256, clip_param=0.2, entcoeff=0.01,\n",
    "                 optim_epochs=4, optim_stepsize=1e-3, optim_batchsize=64, lam=0.95, adam_epsilon=1e-5,\n",
    "                 schedule='linear', verbose=0, tensorboard_log=None, _init_setup_model=True):\n",
    "\n",
    "        super().__init__(policy=policy, env=env, verbose=verbose, requires_vec_env=False,\n",
    "                         _init_setup_model=_init_setup_model)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.timesteps_per_actorbatch = timesteps_per_actorbatch\n",
    "        self.clip_param = clip_param\n",
    "        self.entcoeff = entcoeff\n",
    "        self.optim_epochs = optim_epochs\n",
    "        self.optim_stepsize = optim_stepsize\n",
    "        self.optim_batchsize = optim_batchsize\n",
    "        self.lam = lam\n",
    "        self.adam_epsilon = adam_epsilon\n",
    "        self.schedule = schedule\n",
    "        self.tensorboard_log = tensorboard_log\n",
    "\n",
    "        self.graph = None\n",
    "        self.sess = None\n",
    "        self.policy_pi = None\n",
    "        self.loss_names = None\n",
    "        self.lossandgrad = None\n",
    "        self.adam = None\n",
    "        self.assign_old_eq_new = None\n",
    "        self.compute_losses = None\n",
    "        self.params = None\n",
    "        self.step = None\n",
    "        self.proba_step = None\n",
    "        self.initial_state = None\n",
    "        self.summary = None\n",
    "        self.episode_reward = None\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self.setup_model()\n",
    "\n",
    "    def setup_model(self):\n",
    "        with SetVerbosity(self.verbose):\n",
    "\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "                self.sess = tf_util.single_threaded_session(graph=self.graph)\n",
    "\n",
    "                # Construct network for new policy\n",
    "                self.policy_pi = self.policy(self.sess, self.observation_space, self.action_space, self.n_envs, 1,\n",
    "                                             None, reuse=False)\n",
    "\n",
    "                # Network for old policy\n",
    "                with tf.variable_scope(\"oldpi\", reuse=False):\n",
    "                    old_pi = self.policy(self.sess, self.observation_space, self.action_space, self.n_envs, 1,\n",
    "                                         None, reuse=False)\n",
    "\n",
    "                with tf.variable_scope(\"loss\", reuse=False):\n",
    "                    # Target advantage function (if applicable)\n",
    "                    atarg = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "                    # Empirical return\n",
    "                    ret = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "\n",
    "                    # learning rate multiplier, updated with schedule\n",
    "                    lrmult = tf.placeholder(name='lrmult', dtype=tf.float32, shape=[])\n",
    "\n",
    "                    # Annealed cliping parameter epislon\n",
    "                    clip_param = self.clip_param * lrmult\n",
    "\n",
    "                    obs_ph = self.policy_pi.obs_ph\n",
    "                    action_ph = self.policy_pi.pdtype.sample_placeholder([None])\n",
    "\n",
    "                    kloldnew = old_pi.proba_distribution.kl(self.policy_pi.proba_distribution)\n",
    "                    ent = self.policy_pi.proba_distribution.entropy()\n",
    "                    meankl = tf.reduce_mean(kloldnew)\n",
    "                    meanent = tf.reduce_mean(ent)\n",
    "                    pol_entpen = (-self.entcoeff) * meanent\n",
    "\n",
    "                    # pnew / pold\n",
    "                    ratio = tf.exp(self.policy_pi.proba_distribution.logp(action_ph) -\n",
    "                                   old_pi.proba_distribution.logp(action_ph))\n",
    "\n",
    "                    # surrogate from conservative policy iteration\n",
    "                    surr1 = ratio * atarg\n",
    "                    surr2 = tf.clip_by_value(ratio, 1.0 - clip_param, 1.0 + clip_param) * atarg\n",
    "\n",
    "                    # PPO's pessimistic surrogate (L^CLIP)\n",
    "                    pol_surr = - tf.reduce_mean(tf.minimum(surr1, surr2))\n",
    "                    vf_loss = tf.reduce_mean(tf.square(self.policy_pi.value_fn[:, 0] - ret))\n",
    "                    total_loss = pol_surr + pol_entpen + vf_loss\n",
    "                    losses = [pol_surr, pol_entpen, vf_loss, meankl, meanent]\n",
    "                    self.loss_names = [\"pol_surr\", \"pol_entpen\", \"vf_loss\", \"kl\", \"ent\"]\n",
    "\n",
    "                    tf.summary.scalar('entropy_loss', pol_entpen)\n",
    "                    tf.summary.scalar('policy_gradient_loss', pol_surr)\n",
    "                    tf.summary.scalar('value_function_loss', vf_loss)\n",
    "                    tf.summary.scalar('approximate_kullback-leiber', meankl)\n",
    "                    tf.summary.scalar('clip_factor', clip_param)\n",
    "                    tf.summary.scalar('loss', total_loss)\n",
    "\n",
    "                    self.params = tf_util.get_trainable_vars(\"model\")\n",
    "\n",
    "                    self.assign_old_eq_new = tf_util.function(\n",
    "                        [], [], updates=[tf.assign(oldv, newv) for (oldv, newv) in\n",
    "                                         zipsame(tf_util.get_globals_vars(\"oldpi\"), tf_util.get_globals_vars(\"model\"))])\n",
    "\n",
    "                with tf.variable_scope(\"Adam_mpi\", reuse=False):\n",
    "                    self.adam = MpiAdam(self.params, epsilon=self.adam_epsilon, sess=self.sess)\n",
    "\n",
    "                with tf.variable_scope(\"input_info\", reuse=False):\n",
    "                    tf.summary.scalar('discounted_rewards', tf.reduce_mean(ret))\n",
    "                    tf.summary.histogram('discounted_rewards', ret)\n",
    "                    tf.summary.scalar('learning_rate', tf.reduce_mean(self.optim_stepsize))\n",
    "                    tf.summary.histogram('learning_rate', self.optim_stepsize)\n",
    "                    tf.summary.scalar('advantage', tf.reduce_mean(atarg))\n",
    "                    tf.summary.histogram('advantage', atarg)\n",
    "                    tf.summary.scalar('clip_range', tf.reduce_mean(self.clip_param))\n",
    "                    tf.summary.histogram('clip_range', self.clip_param)\n",
    "                    #if len(self.observation_space.shape) == 3:\n",
    "                    #    tf.summary.image('observation', obs_ph)\n",
    "                    #else:\n",
    "                    #    tf.summary.histogram('observation', obs_ph)\n",
    "\n",
    "                self.step = self.policy_pi.step\n",
    "                self.proba_step = self.policy_pi.proba_step\n",
    "                self.initial_state = self.policy_pi.initial_state\n",
    "\n",
    "                tf_util.initialize(sess=self.sess)\n",
    "\n",
    "                self.summary = tf.summary.merge_all()\n",
    "\n",
    "                self.lossandgrad = tf_util.function([obs_ph, old_pi.obs_ph, action_ph, atarg, ret, lrmult],\n",
    "                                                    [self.summary, tf_util.flatgrad(total_loss, self.params)] + losses)\n",
    "                self.compute_losses = tf_util.function([obs_ph, old_pi.obs_ph, action_ph, atarg, ret, lrmult],\n",
    "                                                       losses)\n",
    "\n",
    "    def learn(self, total_timesteps, callback=None, seed=None, log_interval=100, tb_log_name=\"PPO1\"):\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name) as writer:\n",
    "            self._setup_learn(seed)\n",
    "\n",
    "            assert issubclass(self.policy, ActorCriticPolicy), \"Error: the input policy for the PPO1 model must be \" \\\n",
    "                                                               \"an instance of common.policies.ActorCriticPolicy.\"\n",
    "\n",
    "            with self.sess.as_default():\n",
    "                self.adam.sync()\n",
    "\n",
    "                # Prepare for rollouts\n",
    "                seg_gen = traj_segment_generator(self.policy_pi, self.env, self.timesteps_per_actorbatch)\n",
    "\n",
    "                episodes_so_far = 0\n",
    "                timesteps_so_far = 0\n",
    "                iters_so_far = 0\n",
    "                t_start = time.time()\n",
    "\n",
    "                # rolling buffer for episode lengths\n",
    "                lenbuffer = deque(maxlen=100)\n",
    "                # rolling buffer for episode rewards\n",
    "                rewbuffer = deque(maxlen=100)\n",
    "\n",
    "                self.episode_reward = np.zeros((self.n_envs,))\n",
    "\n",
    "                while True:\n",
    "                    if callback is not None:\n",
    "                        # Only stop training if return value is False, not when it is None. This is for backwards\n",
    "                        # compatibility with callbacks that have no return statement.\n",
    "                        if callback(locals(), globals()) == False:\n",
    "                            break\n",
    "\n",
    "                    if total_timesteps and timesteps_so_far >= total_timesteps:\n",
    "                        break\n",
    "\n",
    "                    if self.schedule == 'constant':\n",
    "                        cur_lrmult = 1.0\n",
    "                    elif self.schedule == 'linear':\n",
    "                        cur_lrmult = max(1.0 - float(timesteps_so_far) / total_timesteps, 0)\n",
    "                    else:\n",
    "                        raise NotImplementedError\n",
    "\n",
    "                    logger.log(\"********** Iteration %i ************\" % iters_so_far)\n",
    "\n",
    "                    seg = seg_gen.__next__()\n",
    "                    add_vtarg_and_adv(seg, self.gamma, self.lam)\n",
    "\n",
    "                    # ob, ac, atarg, ret, td1ret = map(np.concatenate, (obs, acs, atargs, rets, td1rets))\n",
    "                    obs_ph, action_ph, atarg, tdlamret = seg[\"ob\"], seg[\"ac\"], seg[\"adv\"], seg[\"tdlamret\"]\n",
    "\n",
    "                    # true_rew is the reward without discount\n",
    "                    if writer is not None:\n",
    "                        self.episode_reward = total_episode_reward_logger(self.episode_reward,\n",
    "                                                                          seg[\"true_rew\"].reshape((self.n_envs, -1)),\n",
    "                                                                          seg[\"dones\"].reshape((self.n_envs, -1)),\n",
    "                                                                          writer, timesteps_so_far)\n",
    "\n",
    "                    # predicted value function before udpate\n",
    "                    vpredbefore = seg[\"vpred\"]\n",
    "\n",
    "                    # standardized advantage function estimate\n",
    "                    atarg = (atarg - atarg.mean()) / atarg.std()\n",
    "                    dataset = Dataset(dict(ob=obs_ph, ac=action_ph, atarg=atarg, vtarg=tdlamret),\n",
    "                                      shuffle=not issubclass(self.policy, LstmPolicy))\n",
    "                    optim_batchsize = self.optim_batchsize or obs_ph.shape[0]\n",
    "\n",
    "                    # set old parameter values to new parameter values\n",
    "                    self.assign_old_eq_new(sess=self.sess)\n",
    "                    logger.log(\"Optimizing...\")\n",
    "                    logger.log(fmt_row(13, self.loss_names))\n",
    "\n",
    "                    # Here we do a bunch of optimization epochs over the data\n",
    "                    for k in range(self.optim_epochs):\n",
    "                        # list of tuples, each of which gives the loss for a minibatch\n",
    "                        losses = []\n",
    "                        for i, batch in enumerate(dataset.iterate_once(optim_batchsize)):\n",
    "                            steps = (timesteps_so_far +\n",
    "                                     k * optim_batchsize +\n",
    "                                     int(i * (optim_batchsize / len(dataset.data_map))))\n",
    "                            if writer is not None:\n",
    "                                # run loss backprop with summary, but once every 10 runs save the metadata\n",
    "                                # (memory, compute time, ...)\n",
    "                                if (1 + k) % 10 == 0:\n",
    "                                    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                                    run_metadata = tf.RunMetadata()\n",
    "                                    summary, grad, *newlosses = self.lossandgrad(batch[\"ob\"], batch[\"ob\"], batch[\"ac\"],\n",
    "                                                                                 batch[\"atarg\"], batch[\"vtarg\"],\n",
    "                                                                                 cur_lrmult, sess=self.sess,\n",
    "                                                                                 options=run_options,\n",
    "                                                                                 run_metadata=run_metadata)\n",
    "                                    writer.add_run_metadata(run_metadata, 'step%d' % steps)\n",
    "                                else:\n",
    "                                    summary, grad, *newlosses = self.lossandgrad(batch[\"ob\"], batch[\"ob\"], batch[\"ac\"],\n",
    "                                                                                 batch[\"atarg\"], batch[\"vtarg\"],\n",
    "                                                                                 cur_lrmult, sess=self.sess)\n",
    "                                writer.add_summary(summary, steps)\n",
    "                            else:\n",
    "                                _, grad, *newlosses = self.lossandgrad(batch[\"ob\"], batch[\"ob\"], batch[\"ac\"],\n",
    "                                                                       batch[\"atarg\"], batch[\"vtarg\"], cur_lrmult,\n",
    "                                                                       sess=self.sess)\n",
    "\n",
    "                            self.adam.update(grad, self.optim_stepsize * cur_lrmult)\n",
    "                            losses.append(newlosses)\n",
    "                        logger.log(fmt_row(13, np.mean(losses, axis=0)))\n",
    "\n",
    "                    logger.log(\"Evaluating losses...\")\n",
    "                    losses = []\n",
    "                    for batch in dataset.iterate_once(optim_batchsize):\n",
    "                        newlosses = self.compute_losses(batch[\"ob\"], batch[\"ob\"], batch[\"ac\"], batch[\"atarg\"],\n",
    "                                                        batch[\"vtarg\"], cur_lrmult, sess=self.sess)\n",
    "                        losses.append(newlosses)\n",
    "                    mean_losses, _, _ = mpi_moments(losses, axis=0)\n",
    "                    logger.log(fmt_row(13, mean_losses))\n",
    "                    for (loss_val, name) in zipsame(mean_losses, self.loss_names):\n",
    "                        logger.record_tabular(\"loss_\" + name, loss_val)\n",
    "                    logger.record_tabular(\"ev_tdlam_before\", explained_variance(vpredbefore, tdlamret))\n",
    "\n",
    "                    # local values\n",
    "                    lrlocal = (seg[\"ep_lens\"], seg[\"ep_rets\"])\n",
    "\n",
    "                    # list of tuples\n",
    "                    listoflrpairs = MPI.COMM_WORLD.allgather(lrlocal)\n",
    "                    lens, rews = map(flatten_lists, zip(*listoflrpairs))\n",
    "                    lenbuffer.extend(lens)\n",
    "                    rewbuffer.extend(rews)\n",
    "                    logger.record_tabular(\"EpLenMean\", np.mean(lenbuffer))\n",
    "                    logger.record_tabular(\"EpRewMean\", np.mean(rewbuffer))\n",
    "                    logger.record_tabular(\"EpThisIter\", len(lens))\n",
    "                    episodes_so_far += len(lens)\n",
    "                    timesteps_so_far += MPI.COMM_WORLD.allreduce(seg[\"total_timestep\"])\n",
    "                    iters_so_far += 1\n",
    "                    logger.record_tabular(\"EpisodesSoFar\", episodes_so_far)\n",
    "                    logger.record_tabular(\"TimestepsSoFar\", timesteps_so_far)\n",
    "                    logger.record_tabular(\"TimeElapsed\", time.time() - t_start)\n",
    "                    if self.verbose >= 1 and MPI.COMM_WORLD.Get_rank() == 0:\n",
    "                        logger.dump_tabular()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def save(self, save_path):\n",
    "        data = {\n",
    "            \"gamma\": self.gamma,\n",
    "            \"timesteps_per_actorbatch\": self.timesteps_per_actorbatch,\n",
    "            \"clip_param\": self.clip_param,\n",
    "            \"entcoeff\": self.entcoeff,\n",
    "            \"optim_epochs\": self.optim_epochs,\n",
    "            \"optim_stepsize\": self.optim_stepsize,\n",
    "            \"optim_batchsize\": self.optim_batchsize,\n",
    "            \"lam\": self.lam,\n",
    "            \"adam_epsilon\": self.adam_epsilon,\n",
    "            \"schedule\": self.schedule,\n",
    "            \"verbose\": self.verbose,\n",
    "            \"policy\": self.policy,\n",
    "            \"observation_space\": self.observation_space,\n",
    "            \"action_space\": self.action_space,\n",
    "            \"n_envs\": self.n_envs,\n",
    "            \"_vectorize_action\": self._vectorize_action\n",
    "        }\n",
    "\n",
    "        params = self.sess.run(self.params)\n",
    "\n",
    "        self._save_to_file(save_path, data=data, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env_id, num_timesteps, seed):\n",
    "    rank = MPI.COMM_WORLD.Get_rank()\n",
    "\n",
    "    if rank == 0:\n",
    "        logger.configure()\n",
    "    else:\n",
    "        logger.configure(format_strs=[])\n",
    "    workerseed = seed + 10000 * MPI.COMM_WORLD.Get_rank()\n",
    "    set_global_seeds(workerseed)\n",
    "    env = make_atari(env_id)\n",
    "    \n",
    "    env = bench.Monitor(env, logger.get_dir() and\n",
    "                        os.path.join(logger.get_dir(), str(rank)))\n",
    "    env.seed(workerseed)\n",
    "\n",
    "    env = wrap_deepmind(env)\n",
    "    env.seed(workerseed)\n",
    "    \n",
    "    def callback(_locals, _globals):\n",
    "        global n_steps, best_mean_reward\n",
    "        print(\"Step:\", n_steps)\n",
    "\n",
    "        if (n_steps + 1) % 100 == 0:\n",
    "            _locals['self'].save(\"test_mod_model_{}\".format(env_id))\n",
    "        n_steps += 1\n",
    "\n",
    "    model = PPO1_Mod(CnnPolicy, env, timesteps_per_actorbatch=256, clip_param=0.2, entcoeff=0.01, optim_epochs=4,\n",
    "                 optim_stepsize=1e-3, optim_batchsize=64, gamma=0.99, lam=0.95, schedule='linear', verbose=2,\n",
    "                    tensorboard_log=log_dir)\n",
    "    model.learn(total_timesteps=num_timesteps, callback=callback)\n",
    "    model.save(\"model_{}\".format(env_id))\n",
    "\n",
    "    return model, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /home/ubuntu/ppo_logs/BreakoutNoFrameskip-v4-25\n",
      "Logging to /tmp/openai-2018-12-03-02-34-39-494003\n",
      "Logging to /tmp/openai-2018-12-03-02-34-39-496623\n",
      "Step: 0\n",
      "********** Iteration 0 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00285 |      -0.01384 |       3.89478 |       0.00288 |       1.38353\n",
      "     -0.00598 |      -0.01383 |       0.59409 |       0.00288 |       1.38338\n",
      "     -0.00779 |      -0.01383 |       0.16383 |       0.00358 |       1.38265\n",
      "     -0.00935 |      -0.01381 |       0.13253 |       0.00555 |       1.38072\n",
      "Evaluating losses...\n",
      "     -0.00999 |      -0.01381 |       0.11286 |       0.00558 |       1.38068\n",
      "----------------------------------\n",
      "| EpLenMean       | 52.8         |\n",
      "| EpRewMean       | 1.25         |\n",
      "| EpThisIter      | 4            |\n",
      "| EpisodesSoFar   | 4            |\n",
      "| TimeElapsed     | 3.92         |\n",
      "| TimestepsSoFar  | 256          |\n",
      "| ev_tdlam_before | -0.0116      |\n",
      "| loss_ent        | 1.3806794    |\n",
      "| loss_kl         | 0.005582163  |\n",
      "| loss_pol_entpen | -0.013806794 |\n",
      "| loss_pol_surr   | -0.00998617  |\n",
      "| loss_vf_loss    | 0.112864286  |\n",
      "----------------------------------\n",
      "Step: 1\n",
      "********** Iteration 1 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00056 |      -0.01380 |       0.02850 |      2.70e-05 |       1.38019\n",
      "     -0.00921 |      -0.01375 |       0.02981 |       0.00102 |       1.37542\n",
      "     -0.03276 |      -0.01358 |       0.02251 |       0.00974 |       1.35762\n",
      "     -0.03727 |      -0.01325 |       0.02491 |       0.03204 |       1.32530\n",
      "Evaluating losses...\n",
      "     -0.04052 |      -0.01332 |       0.02294 |       0.02709 |       1.33185\n",
      "----------------------------------\n",
      "| EpLenMean       | 34.1         |\n",
      "| EpRewMean       | 0.667        |\n",
      "| EpThisIter      | 11           |\n",
      "| EpisodesSoFar   | 15           |\n",
      "| TimeElapsed     | 5.45         |\n",
      "| TimestepsSoFar  | 557          |\n",
      "| ev_tdlam_before | -0.0119      |\n",
      "| loss_ent        | 1.3318453    |\n",
      "| loss_kl         | 0.027087895  |\n",
      "| loss_pol_entpen | -0.013318453 |\n",
      "| loss_pol_surr   | -0.04051746  |\n",
      "| loss_vf_loss    | 0.022937424  |\n",
      "----------------------------------\n",
      "Step: 2\n",
      "********** Iteration 2 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00080 |      -0.01332 |       0.02129 |      3.52e-05 |       1.33240\n",
      "     -0.00666 |      -0.01317 |       0.02198 |       0.00137 |       1.31686\n",
      "     -0.02052 |      -0.01263 |       0.02153 |       0.01677 |       1.26332\n",
      "     -0.01818 |      -0.01233 |       0.02191 |       0.03099 |       1.23267\n",
      "Evaluating losses...\n",
      "     -0.02132 |      -0.01247 |       0.02135 |       0.02436 |       1.24677\n",
      "----------------------------------\n",
      "| EpLenMean       | 29.3         |\n",
      "| EpRewMean       | 0.5          |\n",
      "| EpThisIter      | 11           |\n",
      "| EpisodesSoFar   | 26           |\n",
      "| TimeElapsed     | 6.95         |\n",
      "| TimestepsSoFar  | 814          |\n",
      "| ev_tdlam_before | -0.00974     |\n",
      "| loss_ent        | 1.2467709    |\n",
      "| loss_kl         | 0.024360651  |\n",
      "| loss_pol_entpen | -0.012467708 |\n",
      "| loss_pol_surr   | -0.021317948 |\n",
      "| loss_vf_loss    | 0.021346323  |\n",
      "----------------------------------\n",
      "Step: 3\n",
      "********** Iteration 3 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00277 |      -0.01242 |       0.20249 |       0.00021 |       1.24235\n",
      "     -0.01647 |      -0.01202 |       0.18962 |       0.00565 |       1.20222\n",
      "     -0.01906 |      -0.01194 |       0.16538 |       0.01324 |       1.19363\n",
      "     -0.01796 |      -0.01198 |       0.17266 |       0.01825 |       1.19796\n",
      "Evaluating losses...\n",
      "     -0.01815 |      -0.01191 |       0.15822 |       0.02022 |       1.19080\n",
      "----------------------------------\n",
      "| EpLenMean       | 30.8         |\n",
      "| EpRewMean       | 0.614        |\n",
      "| EpThisIter      | 7            |\n",
      "| EpisodesSoFar   | 33           |\n",
      "| TimeElapsed     | 8.45         |\n",
      "| TimestepsSoFar  | 1076         |\n",
      "| ev_tdlam_before | 0.00756      |\n",
      "| loss_ent        | 1.1907969    |\n",
      "| loss_kl         | 0.020219326  |\n",
      "| loss_pol_entpen | -0.011907969 |\n",
      "| loss_pol_surr   | -0.018154519 |\n",
      "| loss_vf_loss    | 0.15822303   |\n",
      "----------------------------------\n",
      "Step: 4\n",
      "********** Iteration 4 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00550 |      -0.01155 |       0.15516 |       0.00250 |       1.15543\n",
      "     -0.00501 |      -0.01084 |       0.17397 |       0.01547 |       1.08380\n",
      "      0.00238 |      -0.01206 |       0.16012 |       0.00396 |       1.20608\n",
      "     -0.01154 |      -0.01185 |       0.14966 |       0.00588 |       1.18467\n",
      "Evaluating losses...\n",
      "     -0.01404 |      -0.01099 |       0.14294 |       0.01853 |       1.09932\n",
      "----------------------------------\n",
      "| EpLenMean       | 31.8         |\n",
      "| EpRewMean       | 0.738        |\n",
      "| EpThisIter      | 7            |\n",
      "| EpisodesSoFar   | 40           |\n",
      "| TimeElapsed     | 9.94         |\n",
      "| TimestepsSoFar  | 1341         |\n",
      "| ev_tdlam_before | -0.00247     |\n",
      "| loss_ent        | 1.0993161    |\n",
      "| loss_kl         | 0.01852917   |\n",
      "| loss_pol_entpen | -0.010993162 |\n",
      "| loss_pol_surr   | -0.014037618 |\n",
      "| loss_vf_loss    | 0.1429419    |\n",
      "----------------------------------\n",
      "Step: 5\n",
      "********** Iteration 5 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00053 |      -0.01046 |       0.16785 |       0.00391 |       1.04600\n",
      "     -0.00045 |      -0.01028 |       0.15183 |       0.00514 |       1.02765\n",
      "     -0.00153 |      -0.01039 |       0.15919 |       0.00385 |       1.03931\n",
      "     -0.00363 |      -0.01055 |       0.15076 |       0.00453 |       1.05524\n",
      "Evaluating losses...\n",
      "     -0.00910 |      -0.01095 |       0.14761 |       0.00492 |       1.09533\n",
      "----------------------------------\n",
      "| EpLenMean       | 30.6         |\n",
      "| EpRewMean       | 0.785        |\n",
      "| EpThisIter      | 10           |\n",
      "| EpisodesSoFar   | 50           |\n",
      "| TimeElapsed     | 11.4         |\n",
      "| TimestepsSoFar  | 1605         |\n",
      "| ev_tdlam_before | 0.00513      |\n",
      "| loss_ent        | 1.095334     |\n",
      "| loss_kl         | 0.0049209343 |\n",
      "| loss_pol_entpen | -0.010953341 |\n",
      "| loss_pol_surr   | -0.009097681 |\n",
      "| loss_vf_loss    | 0.14760919   |\n",
      "----------------------------------\n",
      "Step: 6\n",
      "********** Iteration 6 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00578 |      -0.01040 |       0.40208 |       0.00833 |       1.04023\n",
      "     -0.01456 |      -0.00926 |       0.34934 |       0.03538 |       0.92624\n",
      "     -0.01940 |      -0.01024 |       0.32491 |       0.00895 |       1.02354\n",
      "     -0.01415 |      -0.00944 |       0.29452 |       0.03423 |       0.94431\n",
      "Evaluating losses...\n",
      "     -0.01872 |      -0.00953 |       0.29072 |       0.03178 |       0.95294\n",
      "----------------------------------\n",
      "| EpLenMean       | 31.1         |\n",
      "| EpRewMean       | 0.847        |\n",
      "| EpThisIter      | 4            |\n",
      "| EpisodesSoFar   | 54           |\n",
      "| TimeElapsed     | 12.9         |\n",
      "| TimestepsSoFar  | 1867         |\n",
      "| ev_tdlam_before | -0.0242      |\n",
      "| loss_ent        | 0.9529382    |\n",
      "| loss_kl         | 0.031778947  |\n",
      "| loss_pol_entpen | -0.009529382 |\n",
      "| loss_pol_surr   | -0.018719371 |\n",
      "| loss_vf_loss    | 0.29072145   |\n",
      "----------------------------------\n",
      "Step: 7\n",
      "********** Iteration 7 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "      0.00248 |      -0.00977 |       0.49960 |       0.00130 |       0.97693\n",
      "      0.00168 |      -0.01031 |       0.49807 |       0.00797 |       1.03128\n"
     ]
    }
   ],
   "source": [
    "env_id='BreakoutNoFrameskip-v4'\n",
    "num_timesteps=20000000\n",
    "seed=343\n",
    "best_mean_reward, n_steps = -np.inf, 0\n",
    "\n",
    "base_dir = home + '/ppo_logs'\n",
    "prev = [f for f in os.listdir(base_dir) if env_id in f]\n",
    "log_dir = base_dir + '/{}-{}'.format(env_id, len(prev))\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print('Logging to {}'.format(log_dir))\n",
    "\n",
    "logger.configure()\n",
    "model, env = train(env_id, num_timesteps, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 3.0\n",
      "Iters : 106\n"
     ]
    }
   ],
   "source": [
    "env_id='BreakoutNoFrameskip-v4'\n",
    "model = PPO1_Mod.load('test_base_model_' + env_id)\n",
    "\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env)\n",
    "env.reset()\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "total_reward = 0\n",
    "count = 0\n",
    "frames = []\n",
    "level_count = 5\n",
    "\n",
    "taken = []\n",
    "while True:\n",
    "    actions = model.step(np.array([obs]))\n",
    "    obs, reward, done, info = env.step(actions[0])\n",
    "    taken.append(actions[0])\n",
    "    total_reward += reward\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    d = env.render(mode='rgb_array')\n",
    "    plt.imshow(d)\n",
    "    plt.show()\n",
    "    frames.append(d)\n",
    "        \n",
    "    count += 1\n",
    "    if count > 100:\n",
    "        done = True\n",
    "    if done:\n",
    "        print(\"Reward:\", total_reward)\n",
    "        print(\"Iters :\", count)\n",
    "        level_count -= 1\n",
    "        if level_count < 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3126ea43c8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.ylim((1.5,3.5))\n",
    "plt.xlim((0, 50))\n",
    "\n",
    "plt.suptitle(\"Actions Taken During Breakout\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Action\")\n",
    "\n",
    "plt.plot(base_taken, 'o')\n",
    "plt.plot(mod_taken, '.')\n",
    "\n",
    "plt.yticks([2,3], ['Left', 'Right'])\n",
    "plt.gca().legend(('Base','Modified'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = 100\n",
    "fig = plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.ylim((0, 2.5))\n",
    "plt.suptitle(\"Reward During Training in Breakout\")\n",
    "plt.title(\"(Reward calculated using Rolling Mean with Window = {})\".format(win))\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Average Episode Reward\")\n",
    "plt.plot(df_mod.Step, df_mod.Value.rolling(win, center=False).mean())\n",
    "plt.plot(df_base.Step, df_base.Value.rolling(win, center=False).mean())\n",
    "plt.gca().legend(('Modified','Base'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
