{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import multiprocessing\n",
    "from collections import deque\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common import explained_variance, ActorCriticRLModel, tf_util, SetVerbosity, TensorboardWriter\n",
    "from stable_baselines.common.runners import AbstractEnvRunner\n",
    "from stable_baselines.common.policies import LstmPolicy, ActorCriticPolicy\n",
    "from stable_baselines.a2c.utils import total_episode_reward_logger\n",
    "from stable_baselines.ppo2 import PPO2\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO2(ActorCriticRLModel):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (GPU version).\n",
    "    Paper: https://arxiv.org/abs/1707.06347\n",
    "\n",
    "    :param policy: (ActorCriticPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, CnnLstmPolicy, ...)\n",
    "    :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n",
    "    :param gamma: (float) Discount factor\n",
    "    :param n_steps: (int) The number of steps to run for each environment per update\n",
    "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "    :param ent_coef: (float) Entropy coefficient for the loss caculation\n",
    "    :param learning_rate: (float or callable) The learning rate, it can be a function\n",
    "    :param vf_coef: (float) Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: (float) The maximum value for the gradient clipping\n",
    "    :param lam: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "    :param nminibatches: (int) Number of training minibatches per update. For recurrent policies,\n",
    "        the number of environments run in parallel should be a multiple of nminibatches.\n",
    "    :param noptepochs: (int) Number of epoch when optimizing the surrogate\n",
    "    :param cliprange: (float or callable) Clipping parameter, it can be a function\n",
    "    :param verbose: (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n",
    "    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n",
    "    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, policy, env, gamma=0.99, n_steps=128, ent_coef=0.01, learning_rate=2.5e-4, vf_coef=0.5,\n",
    "                 max_grad_norm=0.5, lam=0.95, nminibatches=4, noptepochs=4, cliprange=0.2, verbose=0,\n",
    "                 tensorboard_log=None, _init_setup_model=True):\n",
    "\n",
    "        super(PPO2, self).__init__(policy=policy, env=env, verbose=verbose, requires_vec_env=True,\n",
    "                                   _init_setup_model=_init_setup_model)\n",
    "\n",
    "        if isinstance(learning_rate, float):\n",
    "            learning_rate = constfn(learning_rate)\n",
    "        else:\n",
    "            assert callable(learning_rate)\n",
    "        if isinstance(cliprange, float):\n",
    "            cliprange = constfn(cliprange)\n",
    "        else:\n",
    "            assert callable(cliprange)\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cliprange = cliprange\n",
    "        self.n_steps = n_steps\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.nminibatches = nminibatches\n",
    "        self.noptepochs = noptepochs\n",
    "        self.tensorboard_log = tensorboard_log\n",
    "\n",
    "        self.graph = None\n",
    "        self.sess = None\n",
    "        self.action_ph = None\n",
    "        self.advs_ph = None\n",
    "        self.rewards_ph = None\n",
    "        self.old_neglog_pac_ph = None\n",
    "        self.old_vpred_ph = None\n",
    "        self.learning_rate_ph = None\n",
    "        self.clip_range_ph = None\n",
    "        self.entropy = None\n",
    "        self.vf_loss = None\n",
    "        self.pg_loss = None\n",
    "        self.approxkl = None\n",
    "        self.clipfrac = None\n",
    "        self.params = None\n",
    "        self._train = None\n",
    "        self.loss_names = None\n",
    "        self.train_model = None\n",
    "        self.act_model = None\n",
    "        self.step = None\n",
    "        self.proba_step = None\n",
    "        self.value = None\n",
    "        self.initial_state = None\n",
    "        self.n_batch = None\n",
    "        self.summary = None\n",
    "        self.episode_reward = None\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self.setup_model()\n",
    "\n",
    "    def setup_model(self):\n",
    "        with SetVerbosity(self.verbose):\n",
    "\n",
    "            assert issubclass(self.policy, ActorCriticPolicy), \"Error: the input policy for the PPO2 model must be \" \\\n",
    "                                                               \"an instance of common.policies.ActorCriticPolicy.\"\n",
    "\n",
    "            self.n_batch = self.n_envs * self.n_steps\n",
    "\n",
    "            n_cpu = multiprocessing.cpu_count()\n",
    "            if sys.platform == 'darwin':\n",
    "                n_cpu //= 2\n",
    "\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "                self.sess = tf_util.make_session(num_cpu=n_cpu, graph=self.graph)\n",
    "\n",
    "                n_batch_step = None\n",
    "                n_batch_train = None\n",
    "                if issubclass(self.policy, LstmPolicy):\n",
    "                    assert self.n_envs % self.nminibatches == 0, \"For recurrent policies, \"\\\n",
    "                        \"the number of environments run in parallel should be a multiple of nminibatches.\"\n",
    "                    n_batch_step = self.n_envs\n",
    "                    n_batch_train = self.n_batch // self.nminibatches\n",
    "\n",
    "                act_model = self.policy(self.sess, self.observation_space, self.action_space, self.n_envs, 1,\n",
    "                                        n_batch_step, reuse=False)\n",
    "                with tf.variable_scope(\"train_model\", reuse=True,\n",
    "                                       custom_getter=tf_util.outer_scope_getter(\"train_model\")):\n",
    "                    train_model = self.policy(self.sess, self.observation_space, self.action_space,\n",
    "                                              self.n_envs // self.nminibatches, self.n_steps, n_batch_train,\n",
    "                                              reuse=True)\n",
    "\n",
    "                with tf.variable_scope(\"loss\", reuse=False):\n",
    "                    self.action_ph = train_model.pdtype.sample_placeholder([None], name=\"action_ph\")\n",
    "                    self.advs_ph = tf.placeholder(tf.float32, [None], name=\"advs_ph\")\n",
    "                    self.rewards_ph = tf.placeholder(tf.float32, [None], name=\"rewards_ph\")\n",
    "                    self.old_neglog_pac_ph = tf.placeholder(tf.float32, [None], name=\"old_neglog_pac_ph\")\n",
    "                    self.old_vpred_ph = tf.placeholder(tf.float32, [None], name=\"old_vpred_ph\")\n",
    "                    self.learning_rate_ph = tf.placeholder(tf.float32, [], name=\"learning_rate_ph\")\n",
    "                    self.clip_range_ph = tf.placeholder(tf.float32, [], name=\"clip_range_ph\")\n",
    "\n",
    "                    neglogpac = train_model.proba_distribution.neglogp(self.action_ph)\n",
    "                    self.entropy = tf.reduce_mean(train_model.proba_distribution.entropy())\n",
    "\n",
    "                    vpred = train_model._value\n",
    "                    vpredclipped = self.old_vpred_ph + tf.clip_by_value(\n",
    "                        train_model._value - self.old_vpred_ph, - self.clip_range_ph, self.clip_range_ph)\n",
    "                    vf_losses1 = tf.square(vpred - self.rewards_ph)\n",
    "                    vf_losses2 = tf.square(vpredclipped - self.rewards_ph)\n",
    "                    self.vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))\n",
    "                    ratio = tf.exp(self.old_neglog_pac_ph - neglogpac)\n",
    "                    pg_losses = -self.advs_ph * ratio\n",
    "                    pg_losses2 = -self.advs_ph * tf.clip_by_value(ratio, 1.0 - self.clip_range_ph, 1.0 +\n",
    "                                                                  self.clip_range_ph)\n",
    "                    self.pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))\n",
    "                    self.approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - self.old_neglog_pac_ph))\n",
    "                    self.clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), self.clip_range_ph)))\n",
    "                    loss = self.pg_loss - self.entropy * self.ent_coef + self.vf_loss * self.vf_coef\n",
    "\n",
    "                    tf.summary.scalar('entropy_loss', self.entropy)\n",
    "                    tf.summary.scalar('policy_gradient_loss', self.pg_loss)\n",
    "                    tf.summary.scalar('value_function_loss', self.vf_loss)\n",
    "                    tf.summary.scalar('approximate_kullback-leiber', self.approxkl)\n",
    "                    tf.summary.scalar('clip_factor', self.clipfrac)\n",
    "                    tf.summary.scalar('loss', loss)\n",
    "\n",
    "                    with tf.variable_scope('model'):\n",
    "                        self.params = tf.trainable_variables()\n",
    "                    grads = tf.gradients(loss, self.params)\n",
    "                    if self.max_grad_norm is not None:\n",
    "                        grads, _grad_norm = tf.clip_by_global_norm(grads, self.max_grad_norm)\n",
    "                    grads = list(zip(grads, self.params))\n",
    "                trainer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph, epsilon=1e-5)\n",
    "                self._train = trainer.apply_gradients(grads)\n",
    "\n",
    "                self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']\n",
    "\n",
    "                with tf.variable_scope(\"input_info\", reuse=False):\n",
    "                    tf.summary.scalar('discounted_rewards', tf.reduce_mean(self.rewards_ph))\n",
    "                    tf.summary.histogram('discounted_rewards', self.rewards_ph)\n",
    "                    tf.summary.scalar('learning_rate', tf.reduce_mean(self.learning_rate_ph))\n",
    "                    tf.summary.histogram('learning_rate', self.learning_rate_ph)\n",
    "                    tf.summary.scalar('advantage', tf.reduce_mean(self.advs_ph))\n",
    "                    tf.summary.histogram('advantage', self.advs_ph)\n",
    "                    tf.summary.scalar('clip_range', tf.reduce_mean(self.clip_range_ph))\n",
    "                    tf.summary.histogram('clip_range', self.clip_range_ph)\n",
    "                    tf.summary.scalar('old_neglog_action_probabilty', tf.reduce_mean(self.old_neglog_pac_ph))\n",
    "                    tf.summary.histogram('old_neglog_action_probabilty', self.old_neglog_pac_ph)\n",
    "                    tf.summary.scalar('old_value_pred', tf.reduce_mean(self.old_vpred_ph))\n",
    "                    tf.summary.histogram('old_value_pred', self.old_vpred_ph)\n",
    "                    if len(self.observation_space.shape) == 3:\n",
    "                        tf.summary.image('observation', train_model.obs_ph)\n",
    "                    else:\n",
    "                        tf.summary.histogram('observation', train_model.obs_ph)\n",
    "\n",
    "                self.train_model = train_model\n",
    "                self.act_model = act_model\n",
    "                self.step = act_model.step\n",
    "                self.proba_step = act_model.proba_step\n",
    "                self.value = act_model.value\n",
    "                self.initial_state = act_model.initial_state\n",
    "                tf.global_variables_initializer().run(session=self.sess)  # pylint: disable=E1101\n",
    "\n",
    "                self.summary = tf.summary.merge_all()\n",
    "\n",
    "    def _train_step(self, learning_rate, cliprange, obs, returns, masks, actions, values, neglogpacs, update,\n",
    "                    writer, states=None):\n",
    "        \"\"\"\n",
    "        Training of PPO2 Algorithm\n",
    "\n",
    "        :param learning_rate: (float) learning rate\n",
    "        :param cliprange: (float) Clipping factor\n",
    "        :param obs: (np.ndarray) The current observation of the environment\n",
    "        :param returns: (np.ndarray) the rewards\n",
    "        :param masks: (np.ndarray) The last masks for done episodes (used in recurent policies)\n",
    "        :param actions: (np.ndarray) the actions\n",
    "        :param values: (np.ndarray) the values\n",
    "        :param neglogpacs: (np.ndarray) Negative Log-likelihood probability of Actions\n",
    "        :param update: (int) the current step iteration\n",
    "        :param writer: (TensorFlow Summary.writer) the writer for tensorboard\n",
    "        :param states: (np.ndarray) For recurrent policies, the internal state of the recurrent model\n",
    "        :return: policy gradient loss, value function loss, policy entropy,\n",
    "                approximation of kl divergence, updated clipping range, training update operation\n",
    "        \"\"\"\n",
    "        advs = returns - values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        td_map = {self.train_model.obs_ph: obs, self.action_ph: actions, self.advs_ph: advs, self.rewards_ph: returns,\n",
    "                  self.learning_rate_ph: learning_rate, self.clip_range_ph: cliprange,\n",
    "                  self.old_neglog_pac_ph: neglogpacs, self.old_vpred_ph: values}\n",
    "        if states is not None:\n",
    "            td_map[self.train_model.states_ph] = states\n",
    "            td_map[self.train_model.masks_ph] = masks\n",
    "\n",
    "        if states is None:\n",
    "            update_fac = self.n_batch // self.nminibatches // self.noptepochs\n",
    "        else:\n",
    "            update_fac = self.n_batch // self.nminibatches // self.noptepochs // self.n_steps\n",
    "\n",
    "        if writer is not None:\n",
    "            # run loss backprop with summary, but once every 10 runs save the metadata (memory, compute time, ...)\n",
    "            if (1 + update) % 10 == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train],\n",
    "                    td_map, options=run_options, run_metadata=run_metadata)\n",
    "                writer.add_run_metadata(run_metadata, 'step%d' % (update * update_fac))\n",
    "            else:\n",
    "                summary, policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train],\n",
    "                    td_map)\n",
    "            writer.add_summary(summary, (update * update_fac))\n",
    "        else:\n",
    "            policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                [self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train], td_map)\n",
    "\n",
    "        return policy_loss, value_loss, policy_entropy, approxkl, clipfrac\n",
    "    \n",
    "    def learn_setup(self, total_timesteps):\n",
    "        self.runner = Runner(env=self.env, model=self, n_steps=self.n_steps, gamma=self.gamma, lam=self.lam)\n",
    "        self.episode_reward = np.zeros((self.n_envs,))\n",
    "        self.ep_info_buf = deque(maxlen=100)\n",
    "        self.t_first_start = time.time()\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.nupdates = self.total_timesteps // self.n_batch\n",
    "        self.update = 0\n",
    "        \n",
    "        return self.runner\n",
    "    \n",
    "    def learn_setup_runnerless(self, total_timesteps):\n",
    "        self.episode_reward = np.zeros((self.n_envs,))\n",
    "        self.ep_info_buf = deque(maxlen=100)\n",
    "        self.t_first_start = time.time()\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.nupdates = self.total_timesteps // self.n_batch\n",
    "        self.update = 0\n",
    "        \n",
    "    def learn_step(self, run):\n",
    "        tb_log_name=\"PPO2\"\n",
    "        log_interval = 1\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name) as writer:\n",
    "            assert self.n_batch % self.nminibatches == 0\n",
    "            n_batch_train = self.n_batch // self.nminibatches\n",
    "            t_start = time.time()\n",
    "            frac = 1.0 - (self.update - 1.0) / self.nupdates\n",
    "            lr_now = self.learning_rate(frac)\n",
    "            cliprangenow = self.cliprange(frac)\n",
    "            # true_reward is the reward without discount\n",
    "            obs, returns, masks, actions, values, neglogpacs, states, ep_infos, true_reward = run\n",
    "            self.ep_info_buf.extend(ep_infos)\n",
    "            mb_loss_vals = []\n",
    "\n",
    "            inds = np.arange(self.n_batch)\n",
    "            for epoch_num in range(self.noptepochs):\n",
    "                np.random.shuffle(inds)\n",
    "                for start in range(0, self.n_batch, n_batch_train):\n",
    "                    timestep = ((self.update * self.noptepochs * self.n_batch + epoch_num * self.n_batch + start) //\n",
    "                                n_batch_train)\n",
    "                    end = start + n_batch_train\n",
    "                    mbinds = inds[start:end]\n",
    "                    slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "                    mb_loss_vals.append(self._train_step(lr_now, cliprangenow, *slices, writer=writer,\n",
    "                                                         update=timestep))\n",
    "\n",
    "            loss_vals = np.mean(mb_loss_vals, axis=0)\n",
    "            t_now = time.time()\n",
    "            fps = int(self.n_batch / (t_now - t_start))\n",
    "\n",
    "            if writer is not None:\n",
    "                self.episode_reward = total_episode_reward_logger(self.episode_reward,\n",
    "                                                                  true_reward.reshape((self.n_envs, self.n_steps)),\n",
    "                                                                  masks.reshape((self.n_envs, self.n_steps)),\n",
    "                                                                  writer, self.update * (self.n_batch + 1))\n",
    "\n",
    "            if self.verbose >= 1 and (self.update % log_interval == 0 or self.update == 1):\n",
    "                explained_var = explained_variance(values, returns)\n",
    "                logger.logkv(\"serial_timesteps\", (self.update + 1) * self.n_steps)\n",
    "                logger.logkv(\"nupdates\", (self.update + 1))\n",
    "                logger.logkv(\"total_timesteps\", (self.update + 1) * self.n_batch)\n",
    "                logger.logkv(\"fps\", fps)\n",
    "                logger.logkv(\"explained_variance\", float(explained_var))\n",
    "                logger.logkv('ep_rewmean', safe_mean([ep_info['r'] for ep_info in self.ep_info_buf]))\n",
    "                logger.logkv('eplenmean', safe_mean([ep_info['l'] for ep_info in self.ep_info_buf]))\n",
    "                logger.logkv('time_elapsed', t_start - self.t_first_start)\n",
    "                for (loss_val, loss_name) in zip(loss_vals, self.loss_names):\n",
    "                    logger.logkv(loss_name, loss_val)\n",
    "                logger.dumpkvs()\n",
    "                \n",
    "            self.update += 1\n",
    "        \n",
    "\n",
    "    def learn(self, total_timesteps, callback=None, seed=None, log_interval=1, tb_log_name=\"PPO2\"):\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name) as writer:\n",
    "            self._setup_learn(seed)\n",
    "\n",
    "            runner = Runner(env=self.env, model=self, n_steps=self.n_steps, gamma=self.gamma, lam=self.lam)\n",
    "            self.episode_reward = np.zeros((self.n_envs,))\n",
    "\n",
    "            ep_info_buf = deque(maxlen=100)\n",
    "            t_first_start = time.time()\n",
    "\n",
    "            nupdates = total_timesteps // self.n_batch\n",
    "            for update in range(1, nupdates + 1):\n",
    "                assert self.n_batch % self.nminibatches == 0\n",
    "                n_batch_train = self.n_batch // self.nminibatches\n",
    "                t_start = time.time()\n",
    "                frac = 1.0 - (update - 1.0) / nupdates\n",
    "                lr_now = self.learning_rate(frac)\n",
    "                cliprangenow = self.cliprange(frac)\n",
    "                # true_reward is the reward without discount\n",
    "                obs, returns, masks, actions, values, neglogpacs, states, ep_infos, true_reward = runner.run([])\n",
    "                ep_info_buf.extend(ep_infos)\n",
    "                mb_loss_vals = []\n",
    "                if states is None:  # nonrecurrent version\n",
    "                    inds = np.arange(self.n_batch)\n",
    "                    for epoch_num in range(self.noptepochs):\n",
    "                        np.random.shuffle(inds)\n",
    "                        for start in range(0, self.n_batch, n_batch_train):\n",
    "                            timestep = ((update * self.noptepochs * self.n_batch + epoch_num * self.n_batch + start) //\n",
    "                                        n_batch_train)\n",
    "                            end = start + n_batch_train\n",
    "                            mbinds = inds[start:end]\n",
    "                            slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "                            mb_loss_vals.append(self._train_step(lr_now, cliprangenow, *slices, writer=writer,\n",
    "                                                                 update=timestep))\n",
    "                else:  # recurrent version\n",
    "                    assert self.n_envs % self.nminibatches == 0\n",
    "                    envinds = np.arange(self.n_envs)\n",
    "                    flatinds = np.arange(self.n_envs * self.n_steps).reshape(self.n_envs, self.n_steps)\n",
    "                    envsperbatch = n_batch_train // self.n_steps\n",
    "                    for epoch_num in range(self.noptepochs):\n",
    "                        np.random.shuffle(envinds)\n",
    "                        for start in range(0, self.n_envs, envsperbatch):\n",
    "                            timestep = ((update * self.noptepochs * self.n_envs + epoch_num * self.n_envs + start) //\n",
    "                                        envsperbatch)\n",
    "                            end = start + envsperbatch\n",
    "                            mb_env_inds = envinds[start:end]\n",
    "                            mb_flat_inds = flatinds[mb_env_inds].ravel()\n",
    "                            slices = (arr[mb_flat_inds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "                            mb_states = states[mb_env_inds]\n",
    "                            mb_loss_vals.append(self._train_step(lr_now, cliprangenow, *slices, update=timestep,\n",
    "                                                                 writer=writer, states=mb_states))\n",
    "\n",
    "                loss_vals = np.mean(mb_loss_vals, axis=0)\n",
    "                t_now = time.time()\n",
    "                fps = int(self.n_batch / (t_now - t_start))\n",
    "\n",
    "                if writer is not None:\n",
    "                    self.episode_reward = total_episode_reward_logger(self.episode_reward,\n",
    "                                                                      true_reward.reshape((self.n_envs, self.n_steps)),\n",
    "                                                                      masks.reshape((self.n_envs, self.n_steps)),\n",
    "                                                                      writer, update * (self.n_batch + 1))\n",
    "\n",
    "                if callback is not None:\n",
    "                    callback(locals(), globals())\n",
    "\n",
    "                if self.verbose >= 1 and (update % log_interval == 0 or update == 1):\n",
    "                    explained_var = explained_variance(values, returns)\n",
    "                    logger.logkv(\"serial_timesteps\", (update + 1) * self.n_steps)\n",
    "                    logger.logkv(\"nupdates\", (update + 1))\n",
    "                    logger.logkv(\"total_timesteps\", (update + 1) * self.n_batch)\n",
    "                    logger.logkv(\"fps\", fps)\n",
    "                    logger.logkv(\"explained_variance\", float(explained_var))\n",
    "                    logger.logkv('ep_rewmean', safe_mean([ep_info['r'] for ep_info in ep_info_buf]))\n",
    "                    logger.logkv('eplenmean', safe_mean([ep_info['l'] for ep_info in ep_info_buf]))\n",
    "                    logger.logkv('time_elapsed', t_start - t_first_start)\n",
    "                    for (loss_val, loss_name) in zip(loss_vals, self.loss_names):\n",
    "                        logger.logkv(loss_name, loss_val)\n",
    "                    logger.dumpkvs()\n",
    "\n",
    "            return self\n",
    "\n",
    "    def save(self, save_path):\n",
    "        data = {\n",
    "            \"gamma\": self.gamma,\n",
    "            \"n_steps\": self.n_steps,\n",
    "            \"vf_coef\": self.vf_coef,\n",
    "            \"ent_coef\": self.ent_coef,\n",
    "            \"max_grad_norm\": self.max_grad_norm,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"lam\": self.lam,\n",
    "            \"nminibatches\": self.nminibatches,\n",
    "            \"noptepochs\": self.noptepochs,\n",
    "            \"cliprange\": self.cliprange,\n",
    "            \"verbose\": self.verbose,\n",
    "            \"policy\": self.policy,\n",
    "            \"observation_space\": self.observation_space,\n",
    "            \"action_space\": self.action_space,\n",
    "            \"n_envs\": self.n_envs,\n",
    "            \"_vectorize_action\": self._vectorize_action\n",
    "        }\n",
    "\n",
    "        params = self.sess.run(self.params)\n",
    "\n",
    "        self._save_to_file(save_path, data=data, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, num_obs=2):\n",
    "        self.episode = 0\n",
    "        self.num_obs = num_obs\n",
    "        self.action_space = spaces.MultiBinary(self.num_obs)\n",
    "        self.observation_space = spaces.MultiBinary(self.num_obs)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.episode += 1\n",
    "        self.reward = 0\n",
    "        self.count = 0\n",
    "        self.state = np.random.randint(0,2, (self.num_obs, ))\n",
    "        #self.state = np.array(0)\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 0.\n",
    "        for n in range(self.num_obs):\n",
    "            if action[n] == self.state[n]:\n",
    "                reward += 1./self.num_obs\n",
    "            else:\n",
    "                reward -= 1./self.num_obs\n",
    "            \n",
    "        self.reward += reward\n",
    "            \n",
    "        self.state = np.array(np.random.randint(0,2, (self.num_obs, )))\n",
    "        self.count += 1\n",
    "        done = False\n",
    "        \n",
    "        if self.count > 10:\n",
    "            done = True\n",
    "        return self.state, reward, done, [{'episode': {'r': self.reward, 'l': self.count}}]\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class SubEnvironment:\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, num_obs=2):\n",
    "        self.num_obs = num_obs\n",
    "        self.action_space = spaces.MultiBinary(1)\n",
    "        self.observation_space = spaces.MultiBinary(self.num_obs)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.count = 0\n",
    "        self.reward = 0.\n",
    "        return np.array(self.state)\n",
    "    \n",
    "    def step(self, state, reward, done):\n",
    "        self.reward += reward\n",
    "        self.count += 1\n",
    "        \n",
    "        return state, reward, done, [{'episode': {'r': self.reward, 'l': self.count}}]\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure\n",
    "\n",
    "- We first initialize each submodel with the subenvironment\n",
    "- We then initialize the main environment and reset it\n",
    "- Then, for `train_steps`, loop through each submodel\n",
    "    - Pass the current observation to the runner\n",
    "    - Capture the actions, rewards, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 0\n",
      "R -0.03125\n",
      "Train Step: 1\n",
      "R -0.015625\n",
      "Train Step: 2\n",
      "R -0.0078125\n",
      "Train Step: 3\n",
      "R -0.046875\n",
      "Train Step: 4\n",
      "R 0.0078125\n",
      "Train Step: 5\n",
      "R -0.0390625\n",
      "Train Step: 6\n",
      "R 0.09375\n",
      "Train Step: 7\n",
      "R 0.0546875\n",
      "Train Step: 8\n",
      "R -0.0234375\n",
      "Train Step: 9\n",
      "R -0.0234375\n",
      "Train Step: 10\n",
      "R -0.03125\n",
      "Train Step: 11\n",
      "R 0.171875\n",
      "Train Step: 12\n",
      "R -0.03125\n",
      "Train Step: 13\n",
      "R -0.0390625\n",
      "Train Step: 14\n",
      "R -0.0859375\n",
      "Train Step: 15\n",
      "R -0.0390625\n",
      "Train Step: 16\n",
      "R 0.0234375\n",
      "Train Step: 17\n",
      "R 0.0078125\n",
      "Train Step: 18\n",
      "R 0.1171875\n",
      "Train Step: 19\n",
      "R 0.03125\n",
      "Train Step: 20\n",
      "R -0.046875\n",
      "Train Step: 21\n",
      "R 0.03125\n",
      "Train Step: 22\n",
      "R 0.109375\n",
      "Train Step: 23\n",
      "R 0.171875\n",
      "Train Step: 24\n",
      "R 0.0546875\n",
      "Train Step: 25\n",
      "R -0.015625\n",
      "Train Step: 26\n",
      "R -0.0703125\n",
      "Train Step: 27\n",
      "R 0.1796875\n",
      "Train Step: 28\n",
      "R 0.0234375\n",
      "Train Step: 29\n",
      "R 0.046875\n",
      "Train Step: 30\n",
      "R 0.125\n",
      "Train Step: 31\n",
      "R 0.03125\n",
      "Train Step: 32\n",
      "R 0.0546875\n",
      "Train Step: 33\n",
      "R 0.0234375\n",
      "Train Step: 34\n",
      "R 0.078125\n",
      "Train Step: 35\n",
      "R 0.0625\n",
      "Train Step: 36\n",
      "R 0.0234375\n",
      "Train Step: 37\n",
      "R 0.0078125\n",
      "Train Step: 38\n",
      "R 0.109375\n",
      "Train Step: 39\n",
      "R 0.0\n",
      "Train Step: 40\n",
      "R 0.0234375\n",
      "Train Step: 41\n",
      "R 0.0859375\n",
      "Train Step: 42\n",
      "R -0.015625\n",
      "Train Step: 43\n",
      "R 0.0703125\n",
      "Train Step: 44\n",
      "R -0.0859375\n",
      "Train Step: 45\n",
      "R 0.0625\n",
      "Train Step: 46\n",
      "R 0.0\n",
      "Train Step: 47\n",
      "R 0.1484375\n",
      "Train Step: 48\n",
      "R 0.109375\n",
      "Train Step: 49\n",
      "R -0.015625\n",
      "Train Step: 50\n",
      "R 0.125\n",
      "Train Step: 51\n",
      "R 0.0625\n",
      "Train Step: 52\n",
      "R 0.0390625\n",
      "Train Step: 53\n",
      "R 0.0390625\n",
      "Train Step: 54\n",
      "R 0.046875\n",
      "Train Step: 55\n",
      "R 0.0234375\n",
      "Train Step: 56\n",
      "R -0.0703125\n",
      "Train Step: 57\n",
      "R 0.046875\n",
      "Train Step: 58\n",
      "R 0.109375\n",
      "Train Step: 59\n",
      "R 0.0703125\n",
      "Train Step: 60\n",
      "R 0.0859375\n",
      "Train Step: 61\n",
      "R 0.1328125\n",
      "Train Step: 62\n",
      "R 0.015625\n",
      "Train Step: 63\n",
      "R -0.015625\n",
      "Train Step: 64\n",
      "R 0.0546875\n",
      "Train Step: 65\n",
      "R 0.15625\n",
      "Train Step: 66\n",
      "R -0.015625\n",
      "Train Step: 67\n",
      "R 0.03125\n",
      "Train Step: 68\n",
      "R 0.015625\n",
      "Train Step: 69\n",
      "R 0.0\n",
      "Train Step: 70\n",
      "R 0.0\n",
      "Train Step: 71\n",
      "R 0.109375\n",
      "Train Step: 72\n",
      "R 0.0234375\n",
      "Train Step: 73\n",
      "R 0.0703125\n",
      "Train Step: 74\n",
      "R 0.015625\n",
      "Train Step: 75\n",
      "R -0.0078125\n",
      "Train Step: 76\n",
      "R 0.0\n",
      "Train Step: 77\n",
      "R 0.0859375\n",
      "Train Step: 78\n",
      "R 0.046875\n",
      "Train Step: 79\n",
      "R 0.1015625\n",
      "Train Step: 80\n",
      "R 0.0234375\n",
      "Train Step: 81\n",
      "R 0.1171875\n",
      "Train Step: 82\n",
      "R -0.0078125\n",
      "Train Step: 83\n",
      "R 0.0859375\n",
      "Train Step: 84\n",
      "R 0.0703125\n",
      "Train Step: 85\n",
      "R 0.1328125\n",
      "Train Step: 86\n",
      "R -0.03125\n",
      "Train Step: 87\n",
      "R 0.015625\n",
      "Train Step: 88\n",
      "R 0.0\n",
      "Train Step: 89\n",
      "R 0.0\n",
      "Train Step: 90\n",
      "R 0.1328125\n",
      "Train Step: 91\n",
      "R 0.046875\n",
      "Train Step: 92\n",
      "R 0.0234375\n",
      "Train Step: 93\n",
      "R 0.0078125\n",
      "Train Step: 94\n",
      "R 0.203125\n",
      "Train Step: 95\n",
      "R 0.171875\n",
      "Train Step: 96\n",
      "R 0.171875\n",
      "Train Step: 97\n",
      "R 0.03125\n",
      "Train Step: 98\n",
      "R 0.0390625\n",
      "Train Step: 99\n",
      "R 0.125\n"
     ]
    }
   ],
   "source": [
    "def swap_and_flatten(arr):\n",
    "    shape = arr.shape\n",
    "    return arr.swapaxes(0, 1).reshape(shape[0] * shape[1], *shape[2:])\n",
    "\n",
    "num_subs = 2\n",
    "submodels = []\n",
    "\n",
    "for _ in range(num_subs):\n",
    "    sm = PPO2(MlpPolicy, DummyVecEnv([lambda: SubEnvironment(num_subs)]), verbose=0)\n",
    "    sm.learn_setup_runnerless(10000)\n",
    "    submodels.append(sm)\n",
    "    \n",
    "env = Environment(num_subs)\n",
    "obs = np.zeros((1,) + env.observation_space.shape, dtype=env.observation_space.dtype.name)\n",
    "obs[:] = env.reset()\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "\n",
    "train_steps = 100\n",
    "n_steps = 128\n",
    "average_rewards = []\n",
    "for train_step in range(train_steps):\n",
    "    print(\"Train Step:\", train_step)\n",
    "    \n",
    "    ###\n",
    "    # Create a Run\n",
    "    ###\n",
    "    \n",
    "    # minibatches\n",
    "    mbs = []\n",
    "    for n in range(num_subs):\n",
    "        mbs.append({\n",
    "            'obs': [],\n",
    "            'rewards': [],\n",
    "            'actions': [],\n",
    "            'values': [],\n",
    "            'dones': [],\n",
    "            'neglogpacs': [],\n",
    "            'ep_infos': []\n",
    "        })\n",
    "    \n",
    "    mb_states = None\n",
    "    ep_infos = []\n",
    "    dones = False\n",
    "    train_step_rewards = []\n",
    "    \n",
    "    # for each step\n",
    "    for mb_step in range(n_steps):\n",
    "        augmented_actions = []\n",
    "        # for each submodel\n",
    "        for n, submodel in enumerate(submodels):\n",
    "            # make an action\n",
    "            actions, values, states, neglogpacs = submodel.step(obs)\n",
    "            \n",
    "            # append data\n",
    "            mbs[n]['obs'].append(obs.copy())\n",
    "            mbs[n]['actions'].append(actions)\n",
    "            mbs[n]['values'].append(values)\n",
    "            mbs[n]['neglogpacs'].append(neglogpacs)\n",
    "            mbs[n]['dones'].append([dones])\n",
    "            \n",
    "            # collection the actions\n",
    "            clipped_actions = actions\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                clipped_actions = np.clip(actions, env.action_space.low, env.action_space.high)\n",
    "            augmented_actions.append(clipped_actions)\n",
    "        \n",
    "        # combine the actions\n",
    "        augmented_actions = np.array(augmented_actions).reshape((num_subs, ))\n",
    "        \n",
    "        # step in the environment\n",
    "        obs[:], rewards, dones, infos = env.step(augmented_actions)\n",
    "        \n",
    "        train_step_rewards.append(rewards)\n",
    "        \n",
    "        # for each submodel\n",
    "        for n, submodel in enumerate(submodels):\n",
    "            # append info\n",
    "            for info in infos:\n",
    "                maybeep_info = info.get('episode')\n",
    "                if maybeep_info:\n",
    "                    mbs[n]['ep_infos'].append(maybeep_info)\n",
    "            # append reward\n",
    "            mbs[n]['rewards'].append(rewards)\n",
    "            \n",
    "    # 413 ppo2.py\n",
    "    for n, submodel in enumerate(submodels):\n",
    "        mb_obs = np.asarray(mbs[n]['obs'], dtype=obs.dtype)\n",
    "        mb_rewards = np.asarray(mbs[n]['rewards'], dtype=np.float32)\n",
    "        mb_actions = np.asarray(mbs[n]['actions'])\n",
    "        mb_values = np.asarray(mbs[n]['values'], dtype=np.float32)\n",
    "        mb_neglogpacs = np.asarray(mbs[n]['neglogpacs'], dtype=np.float32)\n",
    "        mb_dones = np.asarray(mbs[n]['dones'], dtype=np.bool)\n",
    "        last_values = submodel.value(obs, None, dones)\n",
    "        \n",
    "        mb_advs = np.zeros_like(mb_rewards)\n",
    "        true_reward = np.copy(mb_rewards)\n",
    "        last_gae_lam = 0\n",
    "        \n",
    "        for step in reversed(range(n_steps)):\n",
    "            if step == n_steps - 1:\n",
    "                nextnonterminal = 1.0 - dones\n",
    "                nextvalues = last_values\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - mb_dones[step + 1]\n",
    "                nextvalues = mb_values[step + 1]\n",
    "            delta = mb_rewards[step] + gamma * nextvalues * nextnonterminal - mb_values[step]\n",
    "            mb_advs[step] = last_gae_lam = delta + gamma * lam * nextnonterminal * last_gae_lam\n",
    "        mb_returns = mb_advs + mb_values\n",
    "        \n",
    "        mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs = \\\n",
    "            map(swap_and_flatten, (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs))\n",
    "\n",
    "        run = mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs, mb_states, ep_infos, true_reward\n",
    "        submodel.learn_step(run)\n",
    "        \n",
    "    average_rewards.append(np.mean(train_step_rewards))\n",
    "    print(\"R\", np.mean(train_step_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(average_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| approxkl           | 9.080186e-07   |\n",
      "| clipfrac           | 0.0            |\n",
      "| ep_rewmean         | 0.27           |\n",
      "| eplenmean          | 6.01           |\n",
      "| explained_variance | 0              |\n",
      "| fps                | 220            |\n",
      "| nupdates           | 1              |\n",
      "| policy_entropy     | 0.6931463      |\n",
      "| policy_loss        | -0.00055283797 |\n",
      "| serial_timesteps   | 128            |\n",
      "| time_elapsed       | 3.81e-06       |\n",
      "| total_timesteps    | 128            |\n",
      "| value_loss         | 1.9819255      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 3.1566447e-06 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_rewmean         | 0.63          |\n",
      "| eplenmean          | 5.97          |\n",
      "| explained_variance | 0             |\n",
      "| fps                | 477           |\n",
      "| nupdates           | 2             |\n",
      "| policy_entropy     | 0.69313496    |\n",
      "| policy_loss        | -0.0010227328 |\n",
      "| serial_timesteps   | 256           |\n",
      "| time_elapsed       | 0.581         |\n",
      "| total_timesteps    | 256           |\n",
      "| value_loss         | 1.3735065     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 1.0828942e-05 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_rewmean         | -0.14         |\n",
      "| eplenmean          | 6.04          |\n",
      "| explained_variance | 0             |\n",
      "| fps                | 545           |\n",
      "| nupdates           | 3             |\n",
      "| policy_entropy     | 0.69307685    |\n",
      "| policy_loss        | -0.0019764411 |\n",
      "| serial_timesteps   | 384           |\n",
      "| time_elapsed       | 0.85          |\n",
      "| total_timesteps    | 384           |\n",
      "| value_loss         | 2.6018732     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 2.8172453e-05 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_rewmean         | 0.08          |\n",
      "| eplenmean          | 6             |\n",
      "| explained_variance | 0             |\n",
      "| fps                | 533           |\n",
      "| nupdates           | 4             |\n",
      "| policy_entropy     | 0.6928716     |\n",
      "| policy_loss        | -0.0030802784 |\n",
      "| serial_timesteps   | 512           |\n",
      "| time_elapsed       | 1.09          |\n",
      "| total_timesteps    | 512           |\n",
      "| value_loss         | 1.5885961     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 7.9015386e-05 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_rewmean         | 0.7           |\n",
      "| eplenmean          | 5.96          |\n",
      "| explained_variance | 0             |\n",
      "| fps                | 512           |\n",
      "| nupdates           | 5             |\n",
      "| policy_entropy     | 0.6922555     |\n",
      "| policy_loss        | -0.0051113544 |\n",
      "| serial_timesteps   | 640           |\n",
      "| time_elapsed       | 1.33          |\n",
      "| total_timesteps    | 640           |\n",
      "| value_loss         | 2.6115475     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00020072889 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_rewmean         | 0.53          |\n",
      "| eplenmean          | 6.03          |\n",
      "| explained_variance | 1.19e-07      |\n",
      "| fps                | 493           |\n",
      "| nupdates           | 6             |\n",
      "| policy_entropy     | 0.69045055    |\n",
      "| policy_loss        | -0.008216398  |\n",
      "| serial_timesteps   | 768           |\n",
      "| time_elapsed       | 1.58          |\n",
      "| total_timesteps    | 768           |\n",
      "| value_loss         | 2.1658962     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00043318587 |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_rewmean         | 0.41          |\n",
      "| eplenmean          | 5.99          |\n",
      "| explained_variance | 5.96e-08      |\n",
      "| fps                | 523           |\n",
      "| nupdates           | 7             |\n",
      "| policy_entropy     | 0.6858989     |\n",
      "| policy_loss        | -0.012361671  |\n",
      "| serial_timesteps   | 896           |\n",
      "| time_elapsed       | 1.84          |\n",
      "| total_timesteps    | 896           |\n",
      "| value_loss         | 2.107726      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0009666163 |\n",
      "| clipfrac           | 0.0          |\n",
      "| ep_rewmean         | 0.99         |\n",
      "| eplenmean          | 5.95         |\n",
      "| explained_variance | 5.96e-08     |\n",
      "| fps                | 491          |\n",
      "| nupdates           | 8            |\n",
      "| policy_entropy     | 0.6756581    |\n",
      "| policy_loss        | -0.017959353 |\n",
      "| serial_timesteps   | 1024         |\n",
      "| time_elapsed       | 2.08         |\n",
      "| total_timesteps    | 1024         |\n",
      "| value_loss         | 2.845972     |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_subs = 1\n",
    "env = DummyVecEnv([lambda: Environment(num_subs)])\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "runner = model.learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_subs = 1\n",
    "env = DummyVecEnv([lambda: Environment(num_subs)])\n",
    "model = PPO2(MlpPolicy, env, verbose=1)\n",
    "\n",
    "runner = model.learn_setup(10000)\n",
    "submodels = [PPO2(MlpPolicy, DummyVecEnv([lambda: SubEnvironment(num_subs)]), verbose=0)]*num_subs\n",
    "\n",
    "for sm in submodels:\n",
    "    sm.learn_setup_runnerless(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "A [[0.]]\n",
      "V [0.]\n",
      "S None\n",
      "N [0.6931472]\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-dedcbe204755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-ed15cb7a4c8d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, models)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# TODO: MOVE THIS OUTSIDE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mmaybeep_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/stable_baselines/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_rews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(i)\n",
    "    run = runner.run(submodels)\n",
    "    for n, sm in enumerate(submodels):\n",
    "        srun = list(run)\n",
    "        srun[3] = srun[3][:,n].reshape(len(srun[3]), 1)\n",
    "        sm.learn_step(srun)\n",
    "    model.learn_step(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ffc3ea50c671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-fefa89da455c>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "model = submodels[0]\n",
    "for i in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    total_reward += rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n",
      "[1] [[1.]]\n",
      "[0] [[0.]]\n",
      "[0] [[0.]]\n",
      "[1] [[1.]]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "total_reward = 0\n",
    "for i in range(100):\n",
    "    action = []\n",
    "    for sm in submodels:\n",
    "        action.append(sm.predict(obs, deterministic=True)[0])\n",
    "    action = np.array([np.array(action).reshape(num_subs,)])\n",
    "    print(obs, action)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    total_reward += rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
